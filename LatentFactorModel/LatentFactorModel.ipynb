{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "view-in-github"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/vitutorial/exercises/blob/master/LatentFactorModel/LatentFactorModel.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "AcYW7MDKScQ6"
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import os\n",
    "import re\n",
    "import urllib.request\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import matplotlib.pyplot as plt\n",
    "import itertools\n",
    "\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.nn.utils.rnn import pack_padded_sequence, pad_packed_sequence\n",
    "device = torch.device(\"cuda:0\") if torch.cuda.is_available() else torch.device(\"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "pMktCZC3Vwyg"
   },
   "source": [
    "In this notebook you will work with a deep generative language model that maps words from a discrete (bit-vector-valued) latent space. We will use text data (we will work on the character level) in Spanish and pytorch. \n",
    "\n",
    "The first section concerns data manipulation and data loading classes necessary for our implementation. You do not need to modify anything in this part of the code."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ca_L1zfDScRC"
   },
   "source": [
    "Let's first download the SIGMORPHON dataset that we will be using for this notebook: these are inflected Spanish words together with some morphosyntactic descriptors. For this notebook we will ignore the morphosyntactic descriptors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 51
    },
    "colab_type": "code",
    "id": "pYl0oQJVScRE",
    "outputId": "1b245b2d-d956-4590-ded1-7d99d88ded0b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading data files...\n",
      "Download complete.\n"
     ]
    }
   ],
   "source": [
    "url = \"https://raw.githubusercontent.com/ryancotterell/sigmorphon2016/master/data/\"\n",
    "train_file = \"spanish-task1-train\"\n",
    "val_file = \"spanish-task1-dev\"\n",
    "test_file = \"spanish-task1-test\"\n",
    "\n",
    "print(\"Downloading data files...\")\n",
    "if not os.path.isfile(train_file):\n",
    "    urllib.request.urlretrieve(url + train_file, filename=train_file)\n",
    "if not os.path.isfile(val_file):\n",
    "    urllib.request.urlretrieve(url + val_file, filename=val_file)\n",
    "if not os.path.isfile(test_file):\n",
    "    urllib.request.urlretrieve(url + test_file, filename=test_file)\n",
    "print(\"Download complete.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "oPlLBwayScRN"
   },
   "source": [
    "# Data\n",
    "\n",
    "In order to work with text data, we need to transform the text into something that our algorithms can work with. The first step of this process is converting words into word ids. We do this by constructing a vocabulary from the data, assigning a new word id to each new word it encounters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "S4wtwvwaScRP"
   },
   "outputs": [],
   "source": [
    "UNK_TOKEN = \"?\"\n",
    "PAD_TOKEN = \"_\"\n",
    "SOW_TOKEN = \">\"\n",
    "EOW_TOKEN = \".\"\n",
    "\n",
    "def extract_inflected_word(s):\n",
    "    \"\"\"\n",
    "    Extracts the inflected words in the SIGMORPHON dataset.\n",
    "    \"\"\"\n",
    "    return s.split()[-1]\n",
    "\n",
    "class Vocabulary:\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.idx_to_char = {0: UNK_TOKEN, 1: PAD_TOKEN, 2: SOW_TOKEN, 3: EOW_TOKEN}\n",
    "        self.char_to_idx = {UNK_TOKEN: 0, PAD_TOKEN: 1, SOW_TOKEN: 2, EOW_TOKEN: 3}\n",
    "        self.word_freqs = {}\n",
    "    \n",
    "    def __getitem__(self, key):\n",
    "        return self.char_to_idx[key] if key in self.char_to_idx else self.char_to_idx[UNK_TOKEN]\n",
    "    \n",
    "    def word(self, idx):\n",
    "        return self.idx_to_char[idx]\n",
    "    \n",
    "    def size(self):\n",
    "        return len(self.char_to_idx)\n",
    "    \n",
    "    @staticmethod\n",
    "    def from_data(filenames):\n",
    "        \"\"\"\n",
    "            Creates a vocabulary from a list of data files. It assumes that the data files have been\n",
    "            tokenized and pre-processed beforehand.\n",
    "        \"\"\"\n",
    "        vocab = Vocabulary()\n",
    "        for filename in filenames:\n",
    "            with open(filename) as f:\n",
    "                for line in f:\n",
    "                    \n",
    "                    # Strip whitespace and the newline symbol.\n",
    "                    word = extract_inflected_word(line.strip())\n",
    "                    \n",
    "                    # Split the words into characters and assign ids to each\n",
    "                    # new character it encounters.\n",
    "                    for char in list(word):\n",
    "                        if char not in vocab.char_to_idx:\n",
    "                            idx = len(vocab.char_to_idx)\n",
    "                            vocab.char_to_idx[char] = idx\n",
    "                            vocab.idx_to_char[idx] = char\n",
    "                            \n",
    "        return vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 51
    },
    "colab_type": "code",
    "id": "5yGmXEQ2TQSJ",
    "outputId": "1837de77-4d4a-4c71-bd04-03c989a5bb39"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Constructing vocabulary...\n",
      "Constructed a vocabulary of 37 types\n"
     ]
    }
   ],
   "source": [
    "# Construct a vocabulary from the training and validation data.\n",
    "print(\"Constructing vocabulary...\")\n",
    "vocab = Vocabulary.from_data([train_file, val_file])\n",
    "print(\"Constructed a vocabulary of %d types\" % vocab.size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 68
    },
    "colab_type": "code",
    "id": "jLP1Yy2_TtzR",
    "outputId": "5f51cef6-ecb7-404a-8397-bce1c8113c9b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "e 8\n",
      "é 24\n",
      "ș 0\n"
     ]
    }
   ],
   "source": [
    "# some examples\n",
    "print('e', vocab['e'])\n",
    "print('é', vocab['é'])\n",
    "print('ș', vocab['ș'])  # something UNKNOWN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "0Wwdq1DtScRX"
   },
   "source": [
    "We also need to load the data files into memory. We create a simple class `TextDataset` that stores the data as a list of words:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "JYC5fXfIScRa"
   },
   "outputs": [],
   "source": [
    "class TextDataset(Dataset):\n",
    "    \"\"\"\n",
    "        A simple class that loads a list of words into memory from a text file,\n",
    "        split by newlines. This does not do any memory optimisation, \n",
    "        so if your dataset is very large, you might want to use an alternative \n",
    "        class.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, text_file, max_len=30):\n",
    "        self.data = []\n",
    "        with open(text_file) as f:\n",
    "            for line in f:\n",
    "                word = extract_inflected_word(line.strip())\n",
    "                if len(list(word)) <= max_len:\n",
    "                    self.data.append(word)\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        return self.data[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 68
    },
    "colab_type": "code",
    "id": "SehavEDQTfIe",
    "outputId": "d9dd2654-8475-455b-8e67-35100c1ebb39"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample from training data: \"compiláramos\"\n",
      "Sample from validation data: \"debutara\"\n",
      "Sample from test data: \"paginabas\"\n"
     ]
    }
   ],
   "source": [
    "# Load the training, validation, and test datasets into memory.\n",
    "train_dataset = TextDataset(train_file)\n",
    "val_dataset = TextDataset(val_file)\n",
    "test_dataset = TextDataset(test_file)\n",
    "\n",
    "# Print some samples from the data:\n",
    "print(\"Sample from training data: \\\"%s\\\"\" % train_dataset[np.random.choice(len(train_dataset))])\n",
    "print(\"Sample from validation data: \\\"%s\\\"\" % val_dataset[np.random.choice(len(val_dataset))])\n",
    "print(\"Sample from test data: \\\"%s\\\"\" % test_dataset[np.random.choice(len(test_dataset))])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "PNQ1TG-3ScRh"
   },
   "source": [
    "Now it's time to write a function that converts a word into a list of character ids using the vocabulary we created before. This function is `create_batch` in the code cell below. This function creates a batch from a list of words, and makes sure that each word starts with a start-of-word symbol and ends with an end-of-word symbol. Because not all words are of equal length in a certain batch, words are padded with padding symbols so that they match the length of the largest word in the batch. The function returns an input batch, an output batch, a mask of 1s for words and 0s for padding symbols, and the sequence lengths of each word in the batch. The output batch is shifted by one character, to reflect the predictions that the model is expected to make. For example, for a word\n",
    "\\begin{align}\n",
    "    \\text{e s p e s e m o s}\n",
    "\\end{align}\n",
    "the input sequence is\n",
    "\\begin{align}\n",
    "    \\text{SOW e s p e s e m o s}\n",
    "\\end{align}\n",
    "and the output sequence is\n",
    "\\begin{align}\n",
    "    \\text{e s p e s e m o s EOW}\n",
    "\\end{align}\n",
    "\n",
    "You can see the output is shifted wrt the input, that's because we will be computing a distribution for the next character in context of its prefix, and that's why we need to shift the sequence this way.\n",
    "\n",
    "\n",
    "Lastly, we create an inverse function `batch_to_words` that recovers the list of words from a padded batch of character ids to use during test time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "GcYfsIaBScRj"
   },
   "outputs": [],
   "source": [
    "def create_batch(words, vocab, device, word_dropout=0.):\n",
    "    \"\"\"\n",
    "    Converts a list of words to a padded batch of word ids. Returns\n",
    "    an input batch, an output batch shifted by one, a sequence mask over\n",
    "    the input batch, and a tensor containing the sequence length of each\n",
    "    batch element.\n",
    "    :param words: a list of words, each a list of token ids\n",
    "    :param vocab: a Vocabulary object for this dataset\n",
    "    :param device: \n",
    "    :param word_dropout: rate at which we omit words from the context (input)\n",
    "    :returns: a batch of padded inputs, a batch of padded outputs, mask, lengths\n",
    "    \"\"\"\n",
    "    tok = np.array([[SOW_TOKEN] + list(w) + [EOW_TOKEN] for w in words])\n",
    "    seq_lengths = [len(w)-1 for w in tok]\n",
    "    max_len = max(seq_lengths)\n",
    "    pad_id = vocab[PAD_TOKEN]\n",
    "    pad_id_input = [\n",
    "        [vocab[w[t]] if t < seq_lengths[idx] else pad_id for t in range(max_len)]\n",
    "            for idx, w in enumerate(tok)]\n",
    "    \n",
    "    # Replace words of the input with <unk> with p = word_dropout.\n",
    "    if word_dropout > 0.:\n",
    "        unk_id = vocab[UNK_TOKEN]\n",
    "        word_drop =  [\n",
    "            [unk_id if (np.random.random() < word_dropout and t < seq_lengths[idx]) else word_ids[t] for t in range(max_len)] \n",
    "                for idx, word_ids in enumerate(pad_id_input)]\n",
    "    \n",
    "    # The output batch is shifted by 1.\n",
    "    pad_id_output = [\n",
    "        [vocab[w[t+1]] if t < seq_lengths[idx] else pad_id for t in range(max_len)]\n",
    "            for idx, w in enumerate(tok)]\n",
    "    \n",
    "    # Convert everything to PyTorch tensors.\n",
    "    batch_input = torch.tensor(pad_id_input)\n",
    "    batch_output = torch.tensor(pad_id_output)\n",
    "    seq_mask = (batch_input != vocab[PAD_TOKEN])\n",
    "    seq_length = torch.tensor(seq_lengths)\n",
    "    \n",
    "    # Move all tensors to the given device.\n",
    "    batch_input = batch_input.to(device)\n",
    "    batch_output = batch_output.to(device)\n",
    "    seq_mask = seq_mask.to(device)\n",
    "    seq_length = seq_length.to(device)\n",
    "    \n",
    "    return batch_input, batch_output, seq_mask, seq_length\n",
    "\n",
    "\n",
    "def batch_to_words(tensors, vocab: Vocabulary):\n",
    "    \"\"\"\n",
    "    Converts a batch of word ids back to words.\n",
    "    :param tensors: [B, T] word ids\n",
    "    :param vocab: a Vocabulary object for this dataset\n",
    "    :returns: an array of strings (each a word).\n",
    "    \"\"\"\n",
    "    words = []\n",
    "    batch_size = tensors.size(0)\n",
    "    for idx in range(batch_size):\n",
    "        word = [vocab.word(t.item()) for t in tensors[idx,:]]\n",
    "        \n",
    "        # Filter out the start-of-word and padding tokens.\n",
    "        word = list(filter(lambda t: t != PAD_TOKEN and t != SOW_TOKEN, word))\n",
    "        \n",
    "        # Remove the end-of-word token and all tokens following it.\n",
    "        if EOW_TOKEN in word:\n",
    "            word = word[:word.index(EOW_TOKEN)]\n",
    "            \n",
    "        words.append(\"\".join(word))\n",
    "    return np.array(words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "l7R5_4wwScRq"
   },
   "source": [
    "In PyTorch the RNN functions expect inputs to be sorted from long words to shorter ones. Therefore we create a simple wrapper class for the DataLoader class that sorts words from long to short:  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Xjtwes5iScRs"
   },
   "outputs": [],
   "source": [
    "class SortingTextDataLoader:\n",
    "    \"\"\"\n",
    "    A wrapper for the DataLoader class that sorts a list of words by their\n",
    "    lengths in descending order.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, dataloader):\n",
    "        self.dataloader = dataloader\n",
    "        self.it = iter(dataloader)\n",
    "    \n",
    "    def __iter__(self):\n",
    "        return self\n",
    "    \n",
    "    def __next__(self):\n",
    "        words = None\n",
    "        for s in self.it:\n",
    "            words = s\n",
    "            break\n",
    "\n",
    "        if words is None:\n",
    "            self.it = iter(self.dataloader)\n",
    "            raise StopIteration\n",
    "        \n",
    "        words = np.array(words)\n",
    "        sort_keys = sorted(range(len(words)), \n",
    "                           key=lambda idx: len(list(words[idx])), \n",
    "                           reverse=True)\n",
    "        sorted_words = words[sort_keys]\n",
    "        return sorted_words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "bkrOx_-NScRw"
   },
   "source": [
    "# Model\n",
    "\n",
    "## Deterministic language model\n",
    "\n",
    "In language modelling, we model a word $x = \\langle x_1, \\ldots, x_n \\rangle$  of length $n = |x|$ as a sequence of categorical draws:\n",
    "\n",
    "\\begin{align}\n",
    "X_i|x_{<i} & \\sim \\text{Cat}(f(x_{<i}; \\theta)) \n",
    "& i = 1, \\ldots, n \\\\\n",
    "\\end{align}\n",
    "\n",
    "where we use $x_{<i}$ to denote a (possibly empty) prefix string, and thus the model makes no Markov assumption. We map from the conditioning context, the prefix $x_{<i}$, to the categorical parameters (a $v$-dimensional probability vector, where $v$ denotes the size of the vocabulary, in this case, the size of the character set) using a fixed neural network architecture whose parameters we collectively denote by $\\theta$.\n",
    "\n",
    "This assigns the following likelihood to the word\n",
    "\\begin{align}\n",
    "    P(x|\\theta) &= \\prod_{i=1}^n P(x_i|x_{<i}, \\theta) \\\\\n",
    "    &= \\prod_{i=1}^n \\text{Cat}(x_i|f(x_{<i}; \\theta))  \n",
    "\\end{align}\n",
    "where the categorical pmf is $\\text{Cat}(k|\\pi) = \\prod_{j=1}^v \\pi_j^{[k=j]} = \\pi_k$. \n",
    "\n",
    "\n",
    "Suppose we have a dataset $\\mathcal D = \\{x^{(1)}, \\ldots, x^{(N)}\\}$ containing $N$ i.i.d. observations. Then we can use the log-likelihood function \n",
    "\\begin{align}\n",
    "\\mathcal L(\\theta|\\mathcal D) &= \\sum_{k=1}^{N} \\log P(x^{(k)}| \\theta) \\\\\n",
    "&= \\sum_{k=1}^{N} \\sum_{i=1}^{|x^{(k)}|} \\log \\text{Cat}(x^{(k)}_i|f(x^{(k)}_{<i}; \\theta))\n",
    "\\end{align}\n",
    " to estimate $\\theta$ by maximisation:\n",
    " \\begin{align}\n",
    " \\theta^\\star = \\arg\\max_{\\theta \\in \\Theta} \\mathcal L(\\theta|\\mathcal D) ~ .\n",
    " \\end{align}\n",
    " \n",
    "\n",
    "We can use stochastic gradient-ascent to find a local optimum of $\\mathcal L(\\theta|\\mathcal D)$, which only requires a gradient estimate:\n",
    "\n",
    "\\begin{align}\n",
    "\\nabla_\\theta \\mathcal L(\\theta|\\mathcal D) &= \\sum_{k=1}^{|\\mathcal D|} \\nabla_\\theta  \\log P(x^{(k)}|\\theta) \\\\ \n",
    "&= \\sum_{k=1}^{|\\mathcal D|} \\frac{1}{N} N \\nabla_\\theta  \\log P(x^{(k)}| \\theta)  \\\\\n",
    "&= \\mathbb E_{\\mathcal U(1/N)} \\left[ N \\nabla_\\theta  \\log P(x^{(K)}| \\theta) \\right]  \\\\\n",
    "&\\overset{\\text{MC}}{\\approx} \\frac{N}{M} \\sum_{m=1}^M \\nabla_\\theta  \\log P(x^{(k_m)}|\\theta) \\\\\n",
    "&\\text{where }K_m \\sim \\mathcal U(1/N)\n",
    "\\end{align}\n",
    "\n",
    "This is a Monte Carlo (MC) estimate of the gradient computed on $M$ data points selected uniformly at random from $\\mathcal D$.\n",
    "\n",
    "For as long as $f$ remains differentiable wrt to its inputs and parameters, we can rely on automatic differentiation to obtain gradient estimates.\n",
    "\n",
    "\n",
    "An example design for $f$ is:\n",
    "\\begin{align}\n",
    "\\mathbf x_i &= \\text{emb}(x_i; \\theta_{\\text{emb}}) \\\\\n",
    "\\mathbf h_0 &= \\mathbf 0 \\\\\n",
    "\\mathbf h_i &= \\text{rnn}(\\mathbf h_{i-1}, \\mathbf x_{i-1}; \\theta_{\\text{rnn}}) \\\\\n",
    "f(x_{<i}; \\theta) &= \\text{softmax}(\\text{dense}_v(\\mathbf h_{i};  \\theta_{\\text{out}}))\n",
    "\\end{align}\n",
    "where \n",
    "* $\\text{emb}$ is a fixed embedding layer with parameters $\\theta_{\\text{emb}}$;\n",
    "* $\\text{rnn}$ is a recurrent architecture with parameters $\\theta_{\\text{rnn}}$, e.g. an LSTM or GRU, and $\\mathbf h_0$ is part of the architecture's parameters;\n",
    "* $\\text{dense}_v$ is a dense layer with $v$ outputs (vocabulary size) and parameters $\\theta_{\\text{out}}$.\n",
    "\n",
    "\n",
    "\n",
    "In what follows we show how to extend this model with a continuous latent word embedding."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "xhO8rD2DYuar"
   },
   "source": [
    "## Deep generative language model\n",
    "\n",
    "We want to model a word $x$ as a draw from the marginal of deep generative model $P(z, x|\\theta, \\alpha) = P(z|\\alpha)P(x|z, \\theta)$. \n",
    "\n",
    "\n",
    "### Generative model\n",
    "\n",
    "The generative story is:\n",
    "\\begin{align}\n",
    "    Z_k & \\sim \\text{Bernoulli}(\\alpha_k) & k=1,\\ldots, K \\\\\n",
    "    X_i | z, x_{<i} &\\sim \\text{Cat}(f(z, x_{<i}; \\theta)) & i=1, \\ldots, n\n",
    "\\end{align}\n",
    "where $z \\in \\mathbb R^K$ and  we impose a product of independent Bernoulli distributions prior. Other choices of prior can induce interesting properties in latent space, for example, the Bernoullis could be correlated, however, in this notebook, we use independent distributions. \n",
    "\n",
    "\n",
    "**About the prior parameter** The parameter of the $k$th Bernoulli distribution is the probability that the $k$th bit in $z$ is set to $1$, and therefore, if we have reasons to believe some bits are more frequent than others (for example, because we expect some bits to capture verb attributes and others to capture noun attributes, and we know nouns are more frequent than verbs) we may be able to have a good guess at $\\alpha_k$ for different $k$, otherwise, we may simply say that bits are about as likely to be on or off a priori, thus setting $\\alpha_k = 0.5$ for every $k$. In this lab, we will treat the prior parameter ($\\alpha$) as *fixed*.\n",
    "\n",
    "**Architecture** It is easy to design $f$ by a simple modification of the deterministic design shown before:\n",
    "\\begin{align}\n",
    "\\mathbf x_i &= \\text{emb}(x_i; \\theta_{\\text{emb}}) \\\\\n",
    "\\mathbf h_0 &= \\tanh(\\text{dense}(z; \\theta_{\\text{init}})) \\\\\n",
    "\\mathbf h_i &= \\text{rnn}(\\mathbf h_{i-1}, \\mathbf x_{i-1}; \\theta_{\\text{rnn}}) \\\\\n",
    "f(x_{<i}; \\theta) &= \\text{softmax}(\\text{dense}_v(\\mathbf h_{i};  \\theta_{\\text{out}}))\n",
    "\\end{align}\n",
    "where we just initialise the recurrent cell using $z$. Note we could also use $z$ in other places, for example, as additional input to every update of the recurrent cell $\\mathbf h_i = \\text{rnn}(\\mathbf h_{i-1}, [\\mathbf x_{i-1}, z])$. This is an architecture choice which like many others can only be judged empirically or on the basis of practical convenience.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "YwB7igyXg8uU"
   },
   "source": [
    "### Parameter estimation\n",
    "\n",
    "The marginal likelihood, necessary for parameter estimation, is now no longer tractable:\n",
    "\\begin{align}\n",
    "P(x|\\theta, \\alpha) &= \\sum_{z \\in \\{0,1\\}^K} P(z|\\alpha)P(x|z, \\theta) \\\\\n",
    "&= \\sum_{z \\in \\{0,1\\}^K} \\prod_{k=1}^K \\text{Bernoulli}(z_k|\\alpha_k)\\prod_{i=1}^n \\text{Cat}(x_i|f(z,x_{<i}; \\theta) ) \n",
    "\\end{align}\n",
    "the intractability is clear as there is an exponential number of assignments to $z$, namely, $2^K$.\n",
    "\n",
    "We turn to variational inference and derive a lowerbound $\\mathcal E(\\theta, \\lambda|\\mathcal D)$ on the log-likelihood function\n",
    "\n",
    "\\begin{align}\n",
    "    \\mathcal E(\\theta, \\lambda|\\mathcal D) &= \\sum_{s=1}^{|\\mathcal D|} \\mathcal E_s(\\theta, \\lambda|x^{(s)}) \n",
    "\\end{align}\n",
    "\n",
    "which for a single datapoint $x$ is\n",
    "\\begin{align}\n",
    "    \\mathcal E(\\theta, \\lambda|x) &= \\mathbb{E}_{Q(z|x, \\lambda)}\\left[\\log P(x|z, \\theta)\\right] - \\text{KL}\\left(Q(z|x, \\lambda)||P(z|\\alpha)\\right)\\\\\n",
    "\\end{align}\n",
    "where we have introduce an independently parameterised auxiliary distribution $Q(z|x, \\lambda)$. The distribution $Q$ which maximises this *evidence lowerbound* (ELBO) is also the distribution that minimises \n",
    "\\begin{align}\n",
    "\\text{KL}(Q(z|x, \\lambda)||P(z|x, \\theta, \\alpha)) = \\mathbb E_{Q(z|x, \\lambda)}\\left[\\log  \\frac{Q(z|x, \\lambda)}{P(z|x, \\theta, \\alpha)}\\right]\n",
    "\\end{align}\n",
    " where $P(z|x, \\theta, \\alpha) = \\frac{P(x, z|\\theta, \\alpha)}{P(x|\\theta, \\alpha)}$ is our intractable true posterior. For that reason, we think of $Q(z|x, \\lambda)$ as an *approximate posterior*. \n",
    " \n",
    " The approximate posterior is an independent model of the latent variable given the data, for that reason we also call it an *inference model*. \n",
    " In this notebook, our inference model will be a product of independent Bernoulli distributions, to make sure that we cover the sample space of our latent variable. We will leave at the end of the notebook as an optional exercise to model correlations (thus achieving *structured* inference, rather than mean field inference). Such mean field (MF) approximation takes $K$ Bernoulli variational factors whose parameters we predict with a neural network:\n",
    " \n",
    "\\begin{align}\n",
    "    Q(z|x, \\lambda) &= \\prod_{k=1}^K \\text{Bernoulli}(z_k|\\beta_k(x; \\lambda))\n",
    "\\end{align}\n",
    " \n",
    "Note we compute a *fixed* number, namely, $K$, of Bernoulli parameters. This can be done with a neural network that outputs $K$ values and employs a sigmoid activation for the outputs.\n",
    " \n",
    " \n",
    "For this choice, the KL term in the ELBO is tractable:\n",
    "\n",
    "\\begin{align}\n",
    "\\text{KL}\\left(Q(z|x, \\lambda)||P(z|\\alpha)\\right) &= \\sum_{k=1}^K \\text{KL}\\left(Q(z_k|x, \\lambda)||P(z_k|\\alpha_k)\\right) \\\\\n",
    "&= \\sum_{k=1}^K \\text{KL}\\left(\\text{Bernoulli}(\\beta_k(x;\\lambda))|| \\text{Bernoulli}(\\alpha_k)\\right) \\\\\n",
    "&= \\sum_{k=1}^K \\beta_k(x;\\lambda) \\log \\frac{\\beta_k(x;\\lambda)}{\\alpha_k} + (1-\\beta_k(x;\\lambda)) \\log \\frac{1-\\beta_k(x;\\lambda)}{1-\\alpha_k}\n",
    "\\end{align}\n",
    "\n",
    "\n",
    " \n",
    "Here's an example design for our inference model:\n",
    "\n",
    "\\begin{align}\n",
    "\\mathbf x_i &= \\text{emb}(x_i; \\lambda_{\\text{emb}}) \\\\\n",
    "\\mathbf f_i &= \\text{rnn}(\\mathbf f_{i-1}, \\mathbf x_{i}; \\lambda_{\\text{fwd}}) \\\\\n",
    "\\mathbf b_i &= \\text{rnn}(\\mathbf b_{i+1}, \\mathbf x_{i}; \\lambda_{\\text{bwd}}) \\\\\n",
    "\\mathbf h &= \\text{dense}([\\mathbf f_{n}, \\mathbf b_1]; \\lambda_{\\text{hid}}) \\\\\n",
    "\\beta(x; \\lambda) &= \\text{sigmoid}(\\text{dense}_K(\\mathbf h; \\lambda_{\\text{out}}))\n",
    "\\end{align}\n",
    "\n",
    "where we use the $\\text{sigmoid}$ activation to make sure our probabilities are independently set between $0$ and $1$. \n",
    " \n",
    "Because we have neural networks compute the Bernoulli variational factors for us, we call this *amortised* mean field inference.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "y8BTCCPVOKp_"
   },
   "source": [
    "### Gradient estimation\n",
    "\n",
    "We have to obtain gradients of the ELBO with respect to $\\theta$ (generative model) and $\\lambda$ (inference model). Recall we will leave $\\alpha$ fixed.\n",
    "\n",
    "For the **generative model**\n",
    "\n",
    "\\begin{align}\n",
    "\\nabla_\\theta \\mathcal E(\\theta, \\lambda|x)  &=\\nabla_\\theta\\sum_{z} Q(z|x, \\lambda)\\log P(x|z,\\theta) - \\underbrace{\\nabla_\\theta \\sum_{k=1}^K \\text{KL}(Q(z_k|x, \\lambda) || P(z_k|\\alpha_k))}_{\\color{blue}{0}}  \\\\\n",
    "&=\\sum_{z} Q(z|x, \\lambda)\\nabla_\\theta\\log P(x|z,\\theta) \\\\\n",
    "&= \\mathbb E_{Q(z|x, \\lambda)}\\left[\\nabla_\\theta\\log P(x|z,\\theta) \\right] \\\\\n",
    "&\\overset{\\text{MC}}{\\approx} \\frac{1}{S} \\sum_{s=1}^S \\nabla_\\theta \\log P(x|z^{(s)}, \\theta) \n",
    "\\end{align}\n",
    "where $z^{(s)} \\sim Q(z|x,\\lambda)$.\n",
    "Note there is no difficulty in obtaining gradient estimates precisely because the samples come from the inference model and therefore do not interfere with backpropagation for updates to $\\theta$.\n",
    "\n",
    "For the **inference model** the story is less straightforward, and we have to use the *score function estimator* (a.k.a. REINFORCE):\n",
    "\n",
    "\\begin{align}\n",
    "\\nabla_\\lambda \\mathcal E(\\theta, \\lambda|x)  &=\\nabla_\\lambda\\sum_{z} Q(z|x, \\lambda)\\log P(x|z,\\theta) - \\nabla_\\lambda \\underbrace{\\sum_{k=1}^K \\text{KL}(Q(z_k|x, \\lambda) || P(z_k|\\alpha_k))}_{ \\color{blue}{\\text{tractable} }}  \\\\\n",
    "&=\\sum_{z} \\nabla_\\lambda Q(z|x, \\lambda)\\log P(x|z,\\theta) - \\sum_{k=1}^K \\nabla_\\lambda \\text{KL}(Q(z_k|x, \\lambda) || P(z_k|\\alpha_k))   \\\\\n",
    "&=\\sum_{z}  \\underbrace{Q(z|x, \\lambda) \\nabla_\\lambda \\log Q(z|x, \\lambda)}_{\\nabla_\\lambda Q(z|x, \\lambda)} \\log P(x|z,\\theta) - \\sum_{k=1}^K \\nabla_\\lambda \\text{KL}(Q(z_k|x, \\lambda) || P(z_k|\\alpha_k))   \\\\\n",
    "&= \\mathbb E_{Q(z|x, \\lambda)}\\left[ \\log P(x|z,\\theta) \\nabla_\\lambda \\log Q(z|x, \\lambda) \\right] - \\sum_{k=1}^K \\nabla_\\lambda \\text{KL}(Q(z_k|x, \\lambda) || P(z_k|\\alpha_k))   \\\\\n",
    "&\\overset{\\text{MC}}{\\approx} \\left(\\frac{1}{S} \\sum_{s=1}^S  \\log P(x|z^{(s)}, \\theta) \\nabla_\\lambda \\log Q(z^{(s)}|x, \\lambda)  \\right) - \\sum_{k=1}^K \\nabla_\\lambda \\text{KL}(Q(z_k|x, \\lambda) || P(z_k|\\alpha_k))  \n",
    "\\end{align}\n",
    "\n",
    "where $z^{(s)} \\sim Q(z|x,\\lambda)$.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "A6lhOGrkTNA3"
   },
   "source": [
    "## Implementation\n",
    "\n",
    "Let's implement the model and the loss (negative ELBO). We work with the notion of a *surrogate loss*, that is, a computation node whose gradients wrt to parameters are equivalent to the gradients we need.\n",
    "\n",
    "For a given sample $z \\sim Q(z|x, \\lambda)$, the following is a single-sample surrogate loss:\n",
    "\n",
    "\\begin{align}\n",
    "\\mathcal S(\\theta, \\lambda|x) = \\log P(x|z, \\theta) + \\color{red}{\\text{detach}(\\log P(x|z, \\theta) )}\\log Q(z|x, \\lambda) - \\sum_{k=1}^K \\text{KL}(Q(z_k|x, \\lambda) || P(z_k|\\alpha_k))\n",
    "\\end{align}\n",
    "\n",
    "Check the documentation of pytorch's `detach` method.\n",
    "\n",
    "Show that it's gradients wrt $\\theta$ and $\\lambda$ are exactly what we need:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ssFmFU9z9gWt"
   },
   "source": [
    "\\begin{align}\n",
    "\\nabla_\\theta \\mathcal S(\\theta, \\lambda|x) = \\color{red}{?}\n",
    "\\end{align}\n",
    "\n",
    "\\begin{align}\n",
    "\\nabla_\\lambda \\mathcal S(\\theta, \\lambda|x) = \\color{red}{?}\n",
    "\\end{align}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "JR02uHBu93j0"
   },
   "source": [
    "Let's now turn to the actual implementation in pytorch of the inference model as well as the generative model. \n",
    "\n",
    "Here and there we will provide helper code for you."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "IC5gZrwr0pWT"
   },
   "outputs": [],
   "source": [
    "def bernoulli_log_probs_from_logits(logits):\n",
    "    \"\"\"\n",
    "    Let p be the Bernoulli parameter and q = 1 - p.\n",
    "    This function is a stable computation of p and q from logit = log(p/q).\n",
    "    :param logit: log (p/q)\n",
    "    :return: log_p, log_q\n",
    "    \"\"\"\n",
    "    return - F.softplus(-logits), - F.softplus(logits)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "beWOhw4t-qJC"
   },
   "source": [
    "We start with the implementation of a product of Bernoulli distributions where the parameters are *given* at construction time. That is, for some vector $b_1, \\ldots, b_K$ we have\n",
    "\\begin{equation}\n",
    "    Z_k \\sim \\text{Bernoulli}(b_k)\n",
    "\\end{equation}\n",
    "and thus the joint probability of $z_1, \\ldots, z_K$ is given by $\\prod_{k=1}^K \\text{Bernoulli}(z_k|b_k)$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ProductOfBernoullis:\n",
    "    \"\"\"\n",
    "    This is class models a product of independent Bernoulli distributions.\n",
    "    \n",
    "    Each product of Bernoulli is defined by a D-dimensional vector of logits\n",
    "    for each independent Bernoulli variable.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, logits):\n",
    "        \"\"\"\n",
    "        :param p: a tensor of D Bernoulli parameters (logits) for each batch element. [B, D]\n",
    "        \"\"\"\n",
    "        pass\n",
    "        \n",
    "    def mean(self):\n",
    "        \"\"\"For Bernoulli variables this is the probability of each Bernoulli being 1.\"\"\"\n",
    "        return None\n",
    "    \n",
    "    def std(self):\n",
    "        \"\"\"For Bernoulli variables this is p*(1-p) where p\n",
    "        is the probability of the Bernoulli being 1\"\"\"\n",
    "        return self.probs * (1.0 - self.probs)\n",
    "    \n",
    "    def sample(self):\n",
    "        \"\"\"\n",
    "        Returns a sample with the shape of the Bernoulli parameter. # [B, D]\n",
    "        \"\"\"\n",
    "        return None\n",
    "        \n",
    "    def log_prob(self, x):\n",
    "        \"\"\"\n",
    "        Assess the log probability mass of x.\n",
    "        \n",
    "        :param x: a tensor of Bernoulli samples (same shape as the Bernoulli parameter) [B, D]\n",
    "        :returns:  tensor of log probabilitie densities [B]\n",
    "        \"\"\"\n",
    "        return None\n",
    "        \n",
    "    \n",
    "    def unstable_kl(self, other: 'Bernoulli'):\n",
    "        \"\"\"\n",
    "        The straightforward implementation of the KL between two Bernoullis.\n",
    "        This implementation is unstable, a stable implementation is provided in\n",
    "        ProductOfBernoullis.kl(self, q)\n",
    "        \n",
    "        :returns: a tensor of KL values with the same shape as the parameters of self.\n",
    "        \"\"\"\n",
    "        \n",
    "        return None\n",
    "    \n",
    "    def kl(self, other: 'Bernoulli'):\n",
    "        \"\"\"\n",
    "        A stable implementation of the KL divergence between two Bernoulli variables.\n",
    "        \n",
    "        :returns: a tensor of KL values with the same shape as the parameters of self.\n",
    "        \"\"\"\n",
    "        return None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "SsgJqripsmDP"
   },
   "source": [
    "Then we should implement the inference model $Q(z | x, \\lambda)$, that is, a module that uses a neural network to map from a data point $x$ to the parameters of a product of Bernoullis.\n",
    "\n",
    "You might want to consult the documentation of \n",
    "* `torch.nn.Embedding`\n",
    "* `torch.nn.LSTM`\n",
    "* `torch.nn.Linear`\n",
    "* and of our own `ProductOfBernoullis` distribution (see above)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "XUqjJzdXYtMa"
   },
   "outputs": [],
   "source": [
    "class InferenceModel(nn.Module):\n",
    "\n",
    "    def __init__(self, vocab_size, embedder, hidden_size,\n",
    "                 latent_size, pad_idx, bidirectional=False):\n",
    "        \"\"\"\n",
    "        Implement the layers in the inference model.\n",
    "        \n",
    "        :param vocab_size: size of the vocabulary of the language\n",
    "        :param embedder: embedding layer\n",
    "        :param hidden_size: size of recurrent cell\n",
    "        :param latent_size: size K of the latent variable\n",
    "        :param pad_idx: id of the -PAD- token\n",
    "        :param bidirectional: whether we condition on x via a bidirectional or \n",
    "          unidirectional encoder          \n",
    "        \"\"\"\n",
    "        super().__init__()  # pytorch modules should always start with this\n",
    "        pass\n",
    "        # Construct your NN blocks here\n",
    "        #  and make sure every block is an attribute of self\n",
    "        #  or they won't get initialised properly\n",
    "        #  for example, self.my_linear_layer = torch.nn.Linear(...)\n",
    "\n",
    "    def forward(self, x, seq_mask, seq_len) -> ProductOfBernoullis:\n",
    "        \"\"\"\n",
    "        Return an inference product of Bernoullis per instance in the mini-batch\n",
    "        :param x: words [B, T] as token ids\n",
    "        :param seq_mask: indicates valid positions vs padding positions [B, T]\n",
    "        :param seq_len: the length of the sequences [B]\n",
    "        :return: a collection of B ProductOfBernoullis approximate posterior, \n",
    "            each a distribution over K-dimensional bit vectors\n",
    "        \"\"\"\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "dc05nZtd3s9l"
   },
   "outputs": [],
   "source": [
    "# tests for inference model\n",
    "pad_idx = vocab.char_to_idx[PAD_TOKEN]\n",
    "\n",
    "dummy_inference_model = InferenceModel(\n",
    "    vocab_size=vocab.size(),\n",
    "    embedder=nn.Embedding(vocab.size(), 64, padding_idx=pad_idx),\n",
    "    hidden_size=128, latent_size=16, pad_idx=pad_idx, bidirectional=True\n",
    ").to(device=device)\n",
    "dummy_batch_size = 32\n",
    "dummy_dataloader = SortingTextDataLoader(DataLoader(train_dataset, batch_size=dummy_batch_size))\n",
    "dummy_words = next(dummy_dataloader)\n",
    "\n",
    "x_in, _, seq_mask, seq_len = create_batch(dummy_words, vocab, device)\n",
    "\n",
    "q_z_given_x = dummy_inference_model.forward(x_in, seq_mask, seq_len)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "EB7rVW7ZsBHP"
   },
   "source": [
    "Then we should implement the generative latent factor model. The decoder is a sequence of correlated Categorical draws that condition on a latent factor assignment. \n",
    "\n",
    "We will be parameterising categorical distributions, so you might want to check the documentation of `torch.distributions.categorical.Categorical`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "LXXS-JDbpRND"
   },
   "outputs": [],
   "source": [
    "from torch.distributions import Categorical\n",
    "\n",
    "class LatentFactorModel(nn.Module):\n",
    "    \n",
    "    def __init__(self, vocab_size, emb_size, hidden_size, latent_size,\n",
    "                 pad_idx, dropout=0.):\n",
    "        \"\"\"\n",
    "        :param vocab_size: size of the vocabulary of the language\n",
    "        :param emb_size: dimensionality of embeddings\n",
    "        :param hidden_size: dimensionality of recurrent cell\n",
    "        :param latent_size: this is D the dimensionality of the latent variable z\n",
    "        :param pad_idx: the id reserved to the -PAD- token\n",
    "        :param dropout: a dropout rate (you can ignore this for now)\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        # Construct your NN blocks here, \n",
    "        #  remember to assign them to attributes of self\n",
    "        pass\n",
    "\n",
    "    def init_hidden(self, z):\n",
    "        \"\"\"\n",
    "        Returns the hidden state of the LSTM initialized with a projection of a given z.\n",
    "        :param z: [B, K]\n",
    "        :returns: [num_layers, B, H] hidden state, [num_layers, B, H] cell state            \n",
    "        \"\"\"\n",
    "        pass\n",
    "    \n",
    "    def step(self, prev_x, z, hidden):\n",
    "        \"\"\"\n",
    "        Performs a single LSTM step for a given previous word and hidden state.\n",
    "        Returns the unnormalized log probabilities (logits) over the vocabulary \n",
    "        for this time step. \n",
    "        \n",
    "        :param prev_x: [B, 1] id of the previous token\n",
    "        :param z: [B, K] latent variable\n",
    "        :param hidden:  hidden ([num_layers, B, H] state, [num_layers, B, H] cell)\n",
    "        :returns: [B, V] logits, ([num_layers, B, H] updated state, [num_layers, B, H] updated cell)\n",
    "        \"\"\"\n",
    "        pass\n",
    "        \n",
    "    def forward(self, x, z) -> Categorical:\n",
    "        \"\"\"\n",
    "        Performs an entire forward pass given a sequence of words x and a z.\n",
    "        This returns a collection of [B, T] categorical distributions, each \n",
    "            with support over V events.\n",
    "\n",
    "        :param x: [B, T] token ids \n",
    "        :param z: [B, K] a latent sample\n",
    "        :returns: Categorical object with shape [B,T,V]\n",
    "        \"\"\"\n",
    "        hidden = self.init_hidden(z)\n",
    "        outputs = []\n",
    "        for t in range(x.size(1)):\n",
    "            # [B, 1]\n",
    "            prev_x = x[:, t].unsqueeze(-1)\n",
    "            # logits: [B, V]\n",
    "            logits, hidden = self.step(prev_x, z, hidden)\n",
    "            outputs.append(logits)\n",
    "        outputs = torch.cat(outputs, dim=1)\n",
    "        return Categorical(logits=outputs)\n",
    "        \n",
    "    def loss(self, output_distributions, observations, pz, qz, free_nats=0., evaluation=False):\n",
    "        \"\"\"\n",
    "        Computes the terms in the loss (negative ELBO) given the \n",
    "            output Categorical distributions, observations,\n",
    "        the prior distribution p(z), and the approximate posterior distribution q(z|x).\n",
    "        \n",
    "        If free_nats is nonzero it will clamp the KL divergence between the posterior\n",
    "        and prior to that value, preventing gradient propagation via the KL if it's\n",
    "        below that value. \n",
    "        \n",
    "        If evaluation is set to true, the loss will be summed instead\n",
    "        of averaged over the batch. \n",
    "        \n",
    "        Returns the (surrogate) loss, the ELBO, and the KL. \n",
    "        \n",
    "        :returns: \n",
    "            surrogate loss (scalar),\n",
    "            ELBO (scalar), \n",
    "            KL (scalar)\n",
    "        \"\"\"\n",
    "        pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "53Aztjb5ScR2"
   },
   "source": [
    "The code below is used to assess the model and also investigate what it learned. We implemented it for you, so that you can focus on the VAE part. It's useful however to learn from this example: we do interesting things like computing perplexity and sampling novel words!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "PLfa31jEScR7"
   },
   "source": [
    "# Evaluation metrics\n",
    "\n",
    "During training we'd like to keep track of some evaluation metrics on the validation data in order to keep track of how our model is doing and to perform early stopping. One simple metric we can compute is the ELBO on all the validation or test data using a single sample from the approximate posterior $Q(z|x, \\lambda)$:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "URdX_JJ5ScR9"
   },
   "outputs": [],
   "source": [
    "def eval_elbo(model, inference_model, eval_dataset, vocab, device, batch_size=128):\n",
    "    \"\"\"\n",
    "    Computes a single sample estimate of the ELBO on a given dataset.\n",
    "    This returns both the average ELBO and the average KL (for inspection).\n",
    "    \"\"\"\n",
    "    dl = DataLoader(eval_dataset, batch_size=batch_size)\n",
    "    sorted_dl = SortingTextDataLoader(dl)\n",
    "    \n",
    "    # Make sure the model is in evaluation mode (i.e. disable dropout).\n",
    "    model.eval()\n",
    "            \n",
    "    total_ELBO = 0.\n",
    "    total_KL = 0.\n",
    "    num_words = 0\n",
    "        \n",
    "    # We don't need to compute gradients for this.\n",
    "    with torch.no_grad():\n",
    "        for words in sorted_dl:    \n",
    "            x_in, x_out, seq_mask, seq_len = create_batch(words, vocab, device)\n",
    "            \n",
    "            # Infer the approximate posterior and construct the prior.\n",
    "            qz = inference_model(x_in, seq_mask, seq_len)\n",
    "            pz = ProductOfBernoullis(torch.ones_like(qz.probs) * 0.5)\n",
    "            \n",
    "            # Compute the unnormalized probabilities using a single sample from the\n",
    "            # approximate posterior.\n",
    "            z = qz.sample()\n",
    "            # Compute distributions X_i|z, x_{<i}\n",
    "            px_z = model(x_in, z)\n",
    "            \n",
    "            # Compute the reconstruction loss and KL divergence.\n",
    "            loss, ELBO, KL = model.loss(px_z, x_out, pz, qz, z,\n",
    "                                                 free_nats=0.,\n",
    "                                                 evaluation=True)\n",
    "            total_ELBO += ELBO\n",
    "            total_KL += KL\n",
    "            num_words += x_in.size(0)\n",
    "\n",
    "    # Return the average reconstruction loss and KL.\n",
    "    avg_ELBO = total_ELBO / num_words\n",
    "    avg_KL = total_KL / num_words\n",
    "    return avg_ELBO, avg_KL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "LSkSbjXAEKHF",
    "outputId": "8a8b7593-b6dd-42bc-cc4e-eb3d44d5ccee"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(-37.6747, device='cuda:0') tensor(0.5302, device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "dummy_lm = LatentFactorModel(\n",
    "    vocab.size(), emb_size=64, hidden_size=128, \n",
    "    latent_size=16, pad_idx=pad_idx).to(device=device)\n",
    "\n",
    "!head -n 128 {val_file} > ./dummy_dataset\n",
    "dummy_data = TextDataset('./dummy_dataset')\n",
    "dummy_ELBO, dummy_kl = eval_elbo(dummy_lm, dummy_inference_model,\n",
    "                                     dummy_data, vocab, device)\n",
    "print(dummy_ELBO, dummy_kl)\n",
    "assert dummy_kl.item() > 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "tGO7RBYgScR_"
   },
   "source": [
    "\n",
    "A common metric to evaluate language models is the perplexity per word. The perplexity per word for a dataset is defined as:\n",
    "\n",
    "\\begin{align}\n",
    "    \\text{ppl}(\\mathcal{D}|\\theta, \\lambda) = \\exp\\left(-\\frac{1}{\\sum_{k=1}^{|\\mathcal D|} n^{(k)}} \\sum_{k=1}^{|\\mathcal{D}|} \\log P(x^{(k)}|\\theta, \\lambda)\\right) \n",
    "\\end{align}\n",
    "\n",
    "where $n^{(k)} = |x^{(k)}|$ is the number of tokens in a word and $P(x^{(k)}|\\theta, \\lambda)$ is the probability that our model assigns to the datapoint $x^{(k)}$. In order to compute $\\log P(x|\\theta, \\lambda)$ for our model we need to evaluate the marginal:\n",
    "\n",
    "\\begin{align}\n",
    "    P(x|\\theta, \\lambda) = \\sum_{z \\in \\{0, 1\\}^K} P(x|z,\\theta) P(z|\\alpha)\n",
    "\\end{align}\n",
    "\n",
    "As this is summation  cannot be computed in a reasonable amount of time (due to exponential complexity), we have two options: we can use the earlier derived lower-bound on the log-likelihood, which will give us an upper-bound on the perplexity, or we can make an importance sampling estimate using our approximate posterior distribution. The importance sampling (IS) estimate can be done as:\n",
    "\n",
    "\\begin{align}\n",
    "\\hat P(x|\\theta, \\lambda) &\\overset{\\text{IS}}{\\approx} \\frac{1}{S} \\sum_{s=1}^{S} \\frac{P(z^{(s)}|\\alpha)P(x|z^{(s)}, \\theta)}{Q(z^{(s)}|x)} & \\text{where }z^{(s)} \\sim Q(z|x)\n",
    "\\end{align}\n",
    "\n",
    "where $S$ is the number of samples.\n",
    "\n",
    "Then our perplexity becomes:\n",
    "\\begin{align}\n",
    "    &\\frac{1}{\\sum_{k=1}^{|\\mathcal D|} n^{(k)}}  \\sum_{k=1}^{|\\mathcal D|} \\log P(x^{(k)}|\\theta) \\\\\n",
    "    &\\approx \\frac{1}{\\sum_{k=1}^{|\\mathcal D|} n^{(k)}}  \\sum_{k=1}^{|\\mathcal D|} \\log \\frac{1}{S} \\sum_{s=1}^{S} \\frac{P(z^{(s)}|\\alpha)P(x^{(k)}|z^{(s)}, \\theta)}{Q(z^{(s)}|x^{(k)})} \\\\\n",
    "\\end{align}\n",
    "\n",
    "We define the function `eval_perplexity` below that implements this importance sampling estimate:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "VnaIidkwScSA"
   },
   "outputs": [],
   "source": [
    "def eval_perplexity(model, inference_model, eval_dataset, vocab, device, \n",
    "                    n_samples, batch_size=128):\n",
    "    \"\"\"\n",
    "    Estimates the per-word perplexity using importance sampling with the\n",
    "    given number of samples.\n",
    "    \"\"\"\n",
    "    \n",
    "    dl = DataLoader(eval_dataset, batch_size=batch_size)\n",
    "    sorted_dl = SortingTextDataLoader(dl)\n",
    "    \n",
    "    # Make sure the model is in evaluation mode (i.e. disable dropout).\n",
    "    model.eval()\n",
    "    \n",
    "    log_px = 0.\n",
    "    num_predictions = 0\n",
    "    num_words = 0\n",
    "     \n",
    "    # We don't need to compute gradients for this.\n",
    "    with torch.no_grad():\n",
    "        for words in sorted_dl:\n",
    "            x_in, x_out, seq_mask, seq_len = create_batch(words, vocab, device)\n",
    "            \n",
    "            # Infer the approximate posterior and construct the prior.\n",
    "            qz = inference_model(x_in, seq_mask, seq_len)\n",
    "            pz = ProductOfBernoullis(torch.ones_like(qz.probs) * 0.5) # TODO different prior\n",
    "\n",
    "            # Create an array to hold all samples for this batch.\n",
    "            batch_size = x_in.size(0)\n",
    "            log_px_samples = torch.zeros(n_samples, batch_size)\n",
    "            \n",
    "            # Sample log P(x) n_samples times.\n",
    "            for s in range(n_samples):\n",
    "                \n",
    "                # Sample a z^s from the posterior.\n",
    "                z = qz.sample()\n",
    "                \n",
    "                # Compute log P(x^k|z^s)\n",
    "                px_z = model(x_in, z)\n",
    "                # [B, T]\n",
    "                cond_log_prob = px_z.log_prob(x_out)                \n",
    "                cond_log_prob = torch.where(seq_mask, cond_log_prob, torch.zeros_like(cond_log_prob))\n",
    "                # [B]\n",
    "                cond_log_prob = cond_log_prob.sum(-1)\n",
    "                \n",
    "                # Compute log p(z^s) and log q(z^s|x^k)\n",
    "                prior_log_prob = pz.log_pmf(z) # B\n",
    "                posterior_log_prob = qz.log_pmf(z) # B\n",
    "                \n",
    "                # Store the sample for log P(x^k) importance weighted with p(z^s)/q(z^s|x^k).\n",
    "                log_px_sample = cond_log_prob + prior_log_prob - posterior_log_prob\n",
    "                log_px_samples[s] = log_px_sample\n",
    "                \n",
    "            # Average over the number of samples and count the number of predictions made this batch.\n",
    "            log_px_batch = torch.logsumexp(log_px_samples, dim=0) - \\\n",
    "                    torch.log(torch.Tensor([n_samples]))\n",
    "            log_px += log_px_batch.sum()\n",
    "            num_predictions += seq_len.sum()\n",
    "            num_words += seq_len.size(0)\n",
    "\n",
    "    # Compute and return the perplexity per word.\n",
    "    perplexity = torch.exp(-log_px / num_predictions)\n",
    "    NLL = -log_px / num_words\n",
    "    return perplexity, NLL"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "IvoVmedYScSC"
   },
   "source": [
    "Lastly, we want to occasionally qualitatively see the performance of the model during training, by letting it reconstruct a given word from the latent space. This gives us an idea of whether the model is using the latent space to encode some semantics about the data. For this we use a deterministic greedy decoding algorithm, that chooses the word with maximum probability at every time step, and feeds that word into the next time step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "LDD5XF1GScSC"
   },
   "outputs": [],
   "source": [
    "def greedy_decode(model, z, vocab, max_len=50):\n",
    "    \"\"\"\n",
    "    Greedily decodes a word from a given z, by picking the word with\n",
    "    maximum probability at each time step.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Disable dropout.\n",
    "    model.eval()\n",
    "    \n",
    "    # Don't compute gradients.\n",
    "    with torch.no_grad():\n",
    "        batch_size = z.size(0)\n",
    "        \n",
    "        # We feed the model the start-of-word symbol at the first time step.\n",
    "        prev_x = torch.ones(batch_size, 1, dtype=torch.long).fill_(vocab[SOW_TOKEN]).to(z.device)\n",
    "        \n",
    "        # Initialize the hidden state from z.\n",
    "        hidden = model.init_hidden(z)\n",
    "\n",
    "        predictions = []    \n",
    "        for t in range(max_len):\n",
    "            logits, hidden = model.step(prev_x, z, hidden)\n",
    "            \n",
    "            # Choose the argmax of the unnnormalized probabilities as the\n",
    "            # prediction for this time step.\n",
    "            prediction = torch.argmax(logits, dim=-1)\n",
    "            predictions.append(prediction)\n",
    "            \n",
    "            prev_x = prediction.view(batch_size, 1)\n",
    "            \n",
    "        return torch.cat(predictions, dim=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "GERuGgChScSE"
   },
   "source": [
    "# Training\n",
    "\n",
    "Now it's time to train the model. We use early stopping on the validation perplexity for model selection."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "XH6ocHxaScSF",
    "outputId": "19bcd026-8554-4258-83e7-f12a4b1b2223"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1) step 0: training ELBO (KL) = -39.02 (0.43) -- KL weight = 1.00 -- validation ELBO (KL) = -38.29 (0.43)\n",
      "(1) step 100: training ELBO (KL) = -27.68 (1.20) -- KL weight = 1.00 -- validation ELBO (KL) = -23.76 (1.28)\n",
      "Finished epoch 1\n",
      "Evaluation epoch 1:\n",
      " - validation perplexity: 7.88\n",
      " - validation NLL: 21.97\n",
      " - validation ELBO (KL) = -22.52 (1.25)\n",
      "-- Original word: \"interpretarían\"\n",
      "-- Model reconstruction: \"acontaren\"\n",
      "(2) step 200: training ELBO (KL) = -24.03 (1.33) -- KL weight = 1.00 -- validation ELBO (KL) = -22.47 (1.23)\n",
      "(2) step 300: training ELBO (KL) = -23.19 (1.33) -- KL weight = 1.00 -- validation ELBO (KL) = -22.19 (1.47)\n",
      "Finished epoch 2\n",
      "Evaluation epoch 2:\n",
      " - validation perplexity: 7.41\n",
      " - validation NLL: 21.32\n",
      " - validation ELBO (KL) = -21.99 (1.57)\n",
      "-- Original word: \"subtítulos\"\n",
      "-- Model reconstruction: \"acarrarían\"\n",
      "(3) step 400: training ELBO (KL) = -23.07 (1.66) -- KL weight = 1.00 -- validation ELBO (KL) = -22.02 (1.65)\n",
      "(3) step 500: training ELBO (KL) = -23.00 (1.85) -- KL weight = 1.00 -- validation ELBO (KL) = -22.06 (1.91)\n",
      "Finished epoch 3\n",
      "Evaluation epoch 3:\n",
      " - validation perplexity: 7.34\n",
      " - validation NLL: 21.22\n",
      " - validation ELBO (KL) = -22.09 (2.12)\n",
      "-- Original word: \"antojó\"\n",
      "-- Model reconstruction: \"acontaran\"\n",
      "(4) step 600: training ELBO (KL) = -22.87 (1.95) -- KL weight = 1.00 -- validation ELBO (KL) = -22.17 (2.22)\n",
      "(4) step 700: training ELBO (KL) = -23.29 (2.55) -- KL weight = 1.00 -- validation ELBO (KL) = -22.70 (2.85)\n",
      "Finished epoch 4\n",
      "Evaluation epoch 4:\n",
      " - validation perplexity: 7.77\n",
      " - validation NLL: 21.83\n",
      " - validation ELBO (KL) = -22.74 (3.02)\n",
      "-- Original word: \"cosquillearé\"\n",
      "-- Model reconstruction: \"acontaran\"\n",
      "(5) step 800: training ELBO (KL) = -23.54 (2.97) -- KL weight = 1.00 -- validation ELBO (KL) = -22.73 (3.01)\n",
      "(5) step 900: training ELBO (KL) = -23.41 (2.98) -- KL weight = 1.00 -- validation ELBO (KL) = -22.54 (2.93)\n",
      "Finished epoch 5\n",
      "Evaluation epoch 5:\n",
      " - validation perplexity: 7.69\n",
      " - validation NLL: 21.71\n",
      " - validation ELBO (KL) = -22.73 (3.19)\n",
      "-- Original word: \"chutases\"\n",
      "-- Model reconstruction: \"acalaran\"\n",
      "(6) step 1000: training ELBO (KL) = -23.44 (3.05) -- KL weight = 1.00 -- validation ELBO (KL) = -22.70 (3.17)\n",
      "(6) step 1100: training ELBO (KL) = -23.34 (3.12) -- KL weight = 1.00 -- validation ELBO (KL) = -22.49 (3.04)\n",
      "Finished epoch 6\n",
      "Evaluation epoch 6:\n",
      " - validation perplexity: 7.44\n",
      " - validation NLL: 21.37\n",
      " - validation ELBO (KL) = -22.37 (3.00)\n",
      "-- Original word: \"diversificaciones\"\n",
      "-- Model reconstruction: \"acarraría\"\n",
      "(7) step 1200: training ELBO (KL) = -23.31 (3.03) -- KL weight = 1.00 -- validation ELBO (KL) = -22.49 (3.12)\n",
      "(7) step 1300: training ELBO (KL) = -23.24 (3.18) -- KL weight = 1.00 -- validation ELBO (KL) = -22.34 (3.03)\n",
      "Finished epoch 7\n",
      "Evaluation epoch 7:\n",
      " - validation perplexity: 7.37\n",
      " - validation NLL: 21.27\n",
      " - validation ELBO (KL) = -22.33 (3.08)\n",
      "-- Original word: \"entrelazado\"\n",
      "-- Model reconstruction: \"acontaran\"\n",
      "(8) step 1400: training ELBO (KL) = -23.08 (3.04) -- KL weight = 1.00 -- validation ELBO (KL) = -22.40 (3.16)\n",
      "(8) step 1500: training ELBO (KL) = -23.23 (3.27) -- KL weight = 1.00 -- validation ELBO (KL) = -22.68 (3.49)\n",
      "Finished epoch 8\n",
      "Evaluation epoch 8:\n",
      " - validation perplexity: 7.65\n",
      " - validation NLL: 21.66\n",
      " - validation ELBO (KL) = -22.78 (3.63)\n",
      "-- Original word: \"comulgaríamos\"\n",
      "-- Model reconstruction: \"abarraría\"\n",
      "(9) step 1600: training ELBO (KL) = -23.46 (3.54) -- KL weight = 1.00 -- validation ELBO (KL) = -22.72 (3.58)\n",
      "(9) step 1700: training ELBO (KL) = -23.47 (3.69) -- KL weight = 1.00 -- validation ELBO (KL) = -22.88 (3.83)\n",
      "Finished epoch 9\n",
      "Evaluation epoch 9:\n",
      " - validation perplexity: 7.98\n",
      " - validation NLL: 22.11\n",
      " - validation ELBO (KL) = -23.19 (4.19)\n",
      "-- Original word: \"coleccionarás\"\n",
      "-- Model reconstruction: \"acontaran\"\n",
      "(10) step 1800: training ELBO (KL) = -23.82 (4.08) -- KL weight = 1.00 -- validation ELBO (KL) = -23.18 (4.15)\n",
      "(10) step 1900: training ELBO (KL) = -23.79 (4.08) -- KL weight = 1.00 -- validation ELBO (KL) = -23.11 (4.15)\n",
      "Finished epoch 10\n",
      "Evaluation epoch 10:\n",
      " - validation perplexity: 7.87\n",
      " - validation NLL: 21.96\n",
      " - validation ELBO (KL) = -23.06 (4.13)\n",
      "-- Original word: \"conmemoraran\"\n",
      "-- Model reconstruction: \"acarraría\"\n",
      "(11) step 2000: training ELBO (KL) = -23.79 (4.17) -- KL weight = 1.00 -- validation ELBO (KL) = -23.03 (4.10)\n",
      "(11) step 2100: training ELBO (KL) = -23.59 (3.99) -- KL weight = 1.00 -- validation ELBO (KL) = -22.82 (3.94)\n",
      "Finished epoch 11\n",
      "Evaluation epoch 11:\n",
      " - validation perplexity: 7.71\n",
      " - validation NLL: 21.74\n",
      " - validation ELBO (KL) = -22.98 (4.14)\n",
      "-- Original word: \"esculpieren\"\n",
      "-- Model reconstruction: \"acontaran\"\n",
      "(12) step 2200: training ELBO (KL) = -23.73 (4.10) -- KL weight = 1.00 -- validation ELBO (KL) = -22.90 (4.07)\n",
      "(12) step 2300: training ELBO (KL) = -23.64 (4.15) -- KL weight = 1.00 -- validation ELBO (KL) = -22.97 (4.22)\n",
      "Finished epoch 12\n",
      "Evaluation epoch 12:\n",
      " - validation perplexity: 7.60\n",
      " - validation NLL: 21.59\n",
      " - validation ELBO (KL) = -22.87 (4.16)\n",
      "-- Original word: \"cansándose\"\n",
      "-- Model reconstruction: \"acontrastaría\"\n",
      "(13) step 2400: training ELBO (KL) = -23.68 (4.25) -- KL weight = 1.00 -- validation ELBO (KL) = -23.02 (4.29)\n",
      "(13) step 2500: training ELBO (KL) = -23.56 (4.16) -- KL weight = 1.00 -- validation ELBO (KL) = -22.87 (4.16)\n",
      "Finished epoch 13\n",
      "Evaluation epoch 13:\n",
      " - validation perplexity: 7.78\n",
      " - validation NLL: 21.84\n",
      " - validation ELBO (KL) = -22.99 (4.33)\n",
      "-- Original word: \"desmoldasen\"\n",
      "-- Model reconstruction: \"acontermaría\"\n",
      "(14) step 2600: training ELBO (KL) = -23.61 (4.29) -- KL weight = 1.00 -- validation ELBO (KL) = -23.00 (4.37)\n",
      "(14) step 2700: training ELBO (KL) = -23.76 (4.42) -- KL weight = 1.00 -- validation ELBO (KL) = -23.24 (4.63)\n",
      "Finished epoch 14\n",
      "Evaluation epoch 14:\n",
      " - validation perplexity: 7.79\n",
      " - validation NLL: 21.85\n",
      " - validation ELBO (KL) = -23.09 (4.50)\n",
      "-- Original word: \"homenajearemos\"\n",
      "-- Model reconstruction: \"aconterraría\"\n",
      "(15) step 2800: training ELBO (KL) = -23.89 (4.59) -- KL weight = 1.00 -- validation ELBO (KL) = -23.20 (4.63)\n",
      "(15) step 2900: training ELBO (KL) = -23.97 (4.77) -- KL weight = 1.00 -- validation ELBO (KL) = -23.48 (4.97)\n",
      "Finished epoch 15\n",
      "Evaluation epoch 15:\n",
      " - validation perplexity: 7.99\n",
      " - validation NLL: 22.12\n",
      " - validation ELBO (KL) = -23.23 (4.75)\n",
      "-- Original word: \"pisotearan\"\n",
      "-- Model reconstruction: \"acontaran\"\n",
      "(16) step 3000: training ELBO (KL) = -23.90 (4.76) -- KL weight = 1.00 -- validation ELBO (KL) = -23.19 (4.70)\n",
      "(16) step 3100: training ELBO (KL) = -24.00 (4.88) -- KL weight = 1.00 -- validation ELBO (KL) = -23.60 (5.16)\n",
      "Finished epoch 16\n",
      "Evaluation epoch 16:\n",
      " - validation perplexity: 8.32\n",
      " - validation NLL: 22.56\n",
      " - validation ELBO (KL) = -23.78 (5.34)\n",
      "-- Original word: \"coexistid\"\n",
      "-- Model reconstruction: \"acondiciaren\"\n",
      "(17) step 3200: training ELBO (KL) = -24.46 (5.36) -- KL weight = 1.00 -- validation ELBO (KL) = -24.00 (5.60)\n",
      "(17) step 3300: training ELBO (KL) = -24.72 (5.64) -- KL weight = 1.00 -- validation ELBO (KL) = -23.92 (5.55)\n",
      "Finished epoch 17\n",
      "Evaluation epoch 17:\n",
      " - validation perplexity: 8.33\n",
      " - validation NLL: 22.57\n",
      " - validation ELBO (KL) = -23.87 (5.53)\n",
      "-- Original word: \"ensamblamos\"\n",
      "-- Model reconstruction: \"aconderaría\"\n",
      "(18) step 3400: training ELBO (KL) = -24.50 (5.45) -- KL weight = 1.00 -- validation ELBO (KL) = -23.74 (5.41)\n",
      "(18) step 3500: training ELBO (KL) = -23.51 (4.53) -- KL weight = 1.00 -- validation ELBO (KL) = -23.71 (5.42)\n",
      "Finished epoch 18\n",
      "Evaluation epoch 18:\n",
      " - validation perplexity: 8.29\n",
      " - validation NLL: 22.52\n",
      " - validation ELBO (KL) = -23.95 (5.68)\n",
      "-- Original word: \"caro\"\n",
      "-- Model reconstruction: \"aconternaría\"\n",
      "(19) step 3600: training ELBO (KL) = -24.57 (5.59) -- KL weight = 1.00 -- validation ELBO (KL) = -23.96 (5.73)\n",
      "(19) step 3700: training ELBO (KL) = -24.55 (5.68) -- KL weight = 1.00 -- validation ELBO (KL) = -23.59 (5.36)\n",
      "Finished epoch 19\n",
      "Evaluation epoch 19:\n",
      " - validation perplexity: 8.22\n",
      " - validation NLL: 22.43\n",
      " - validation ELBO (KL) = -23.70 (5.50)\n",
      "-- Original word: \"captáremos\"\n",
      "-- Model reconstruction: \"aconderaría\"\n",
      "(20) step 3800: training ELBO (KL) = -24.24 (5.44) -- KL weight = 1.00 -- validation ELBO (KL) = -23.66 (5.45)\n",
      "(20) step 3900: training ELBO (KL) = -24.25 (5.42) -- KL weight = 1.00 -- validation ELBO (KL) = -23.49 (5.34)\n",
      "Finished epoch 20\n",
      "Evaluation epoch 20:\n",
      " - validation perplexity: 8.11\n",
      " - validation NLL: 22.28\n",
      " - validation ELBO (KL) = -23.60 (5.46)\n",
      "-- Original word: \"endeudado\"\n",
      "-- Model reconstruction: \"acondiciaría\"\n"
     ]
    }
   ],
   "source": [
    "# Define the model hyperparameters.\n",
    "emb_size = 256\n",
    "hidden_size = 256 \n",
    "latent_size = 16\n",
    "bidirectional_encoder = True\n",
    "free_nats = 0 # 5.\n",
    "annealing_steps = 0 # 11400\n",
    "dropout = 0.6\n",
    "word_dropout = 0 # 0.75\n",
    "batch_size = 64\n",
    "learning_rate = 0.001\n",
    "num_epochs = 20\n",
    "n_importance_samples = 3 # 50\n",
    "\n",
    "# Create the training data loader.\n",
    "dl = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "sorted_dl = SortingTextDataLoader(dl)\n",
    "\n",
    "# Create the generative model.\n",
    "model = LatentFactorModel(vocab_size=vocab.size(), \n",
    "                 emb_size=emb_size, \n",
    "                 hidden_size=hidden_size, \n",
    "                 latent_size=latent_size, \n",
    "                 pad_idx=vocab[PAD_TOKEN],\n",
    "                 dropout=dropout)\n",
    "model = model.to(device)\n",
    "\n",
    "# Create the inference model.\n",
    "inference_model = InferenceModel(vocab_size=vocab.size(),\n",
    "                                 embedder=model.embedder,\n",
    "                                 hidden_size=hidden_size,\n",
    "                                 latent_size=latent_size,\n",
    "                                 pad_idx=vocab[PAD_TOKEN],\n",
    "                                 bidirectional=bidirectional_encoder)\n",
    "inference_model = inference_model.to(device)\n",
    "\n",
    "# Create the optimizer.\n",
    "optimizer = optim.Adam(itertools.chain(model.parameters(), \n",
    "                                       inference_model.parameters()), \n",
    "                       lr=learning_rate)\n",
    "\n",
    "# Save the best model (early stopping).\n",
    "best_model = \"./best_model.pt\"\n",
    "best_val_ppl = float(\"inf\")\n",
    "best_epoch = 0\n",
    "\n",
    "# Keep track of some statistics to plot later.\n",
    "train_ELBOs = []\n",
    "train_KLs = []\n",
    "val_ELBOs = []\n",
    "val_KLs = []\n",
    "val_perplexities = []\n",
    "val_NLLs = []\n",
    "\n",
    "step = 0\n",
    "training_ELBO = 0.\n",
    "training_KL = 0.\n",
    "num_batches = 0\n",
    "for epoch_num in range(1, num_epochs+1):    \n",
    "    for words in sorted_dl:\n",
    "\n",
    "        # Make sure the model is in training mode (for dropout).\n",
    "        model.train()\n",
    "\n",
    "        # Transform the words to input, output, seq_len, seq_mask batches.\n",
    "        x_in, x_out, seq_mask, seq_len = create_batch(words, vocab, device,\n",
    "                                                      word_dropout=word_dropout)\n",
    "\n",
    "        # Compute the multiplier for the KL term if we do annealing.\n",
    "        if annealing_steps > 0:\n",
    "            KL_weight = min(1., (1.0 / annealing_steps) * step)\n",
    "        else:\n",
    "            KL_weight = 1.\n",
    "        \n",
    "        # Do a forward pass through the model and compute the training loss. We use\n",
    "        # a reparameterized sample from the approximate posterior during training.\n",
    "        qz = inference_model(x_in, seq_mask, seq_len)\n",
    "        pz = ProductOfBernoullis(torch.ones_like(qz.probs) * 0.5)\n",
    "        z = qz.sample()\n",
    "        px_z = model(x_in, z)\n",
    "        loss, ELBO, KL = model.loss(px_z, x_out, pz, qz, z, free_nats=free_nats)        \n",
    "\n",
    "        # Backpropagate and update the model weights.\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # Update some statistics to track for the training loss.\n",
    "        training_ELBO += ELBO\n",
    "        training_KL += KL\n",
    "        num_batches += 1\n",
    "        \n",
    "        # Every 100 steps we evaluate the model and report progress.\n",
    "        if step % 100 == 0:\n",
    "            val_ELBO, val_KL = eval_elbo(model, inference_model, val_dataset, vocab, device)            \n",
    "            print(\"(%d) step %d: training ELBO (KL) = %.2f (%.2f) --\"\n",
    "                  \" KL weight = %.2f --\"\n",
    "                  \" validation ELBO (KL) = %.2f (%.2f)\" % \n",
    "                  (epoch_num, step, training_ELBO/num_batches, \n",
    "                   training_KL/num_batches, KL_weight, val_ELBO, val_KL))\n",
    "            \n",
    "            # Update some statistics for plotting later.\n",
    "            train_ELBOs.append((step, (training_ELBO/num_batches).item()))\n",
    "            train_KLs.append((step, (training_KL/num_batches).item()))\n",
    "            val_ELBOs.append((step, val_ELBO.item()))\n",
    "            val_KLs.append((step, val_KL.item()))\n",
    "            \n",
    "            # Reset the training statistics.\n",
    "            training_ELBO = 0.\n",
    "            training_KL = 0.\n",
    "            num_batches = 0\n",
    "            \n",
    "        step += 1\n",
    "\n",
    "    # After an epoch we'll compute validation perplexity and save the model\n",
    "    # for early stopping if it's better than previous models.\n",
    "    print(\"Finished epoch %d\" % (epoch_num))\n",
    "    val_perplexity, val_NLL = eval_perplexity(model, inference_model, val_dataset, vocab, device, \n",
    "                                              n_importance_samples)\n",
    "    val_ELBO, val_KL = eval_elbo(model, inference_model, val_dataset, vocab, device)    \n",
    "    \n",
    "    # Keep track of the validation perplexities / NLL.\n",
    "    val_perplexities.append((epoch_num, val_perplexity.item()))\n",
    "    val_NLLs.append((epoch_num, val_NLL.item()))\n",
    "    \n",
    "    # If validation perplexity is better, store this model for early stopping.\n",
    "    if val_perplexity < best_val_ppl:\n",
    "        best_val_ppl = val_perplexity\n",
    "        best_epoch = epoch_num\n",
    "        torch.save(model.state_dict(), best_model)\n",
    "        \n",
    "    # Print epoch statistics.\n",
    "    print(\"Evaluation epoch %d:\\n\"\n",
    "          \" - validation perplexity: %.2f\\n\"\n",
    "          \" - validation NLL: %.2f\\n\"\n",
    "          \" - validation ELBO (KL) = %.2f (%.2f)\"\n",
    "          % (epoch_num, val_perplexity, val_NLL, val_ELBO, val_KL))\n",
    "\n",
    "    # Also show some qualitative results by reconstructing a word from the\n",
    "    # validation data. Use the mean of the approximate posterior and greedy\n",
    "    # decoding.\n",
    "    random_word = val_dataset[np.random.choice(len(val_dataset))]\n",
    "    x_in, _, seq_mask, seq_len = create_batch([random_word], vocab, device)\n",
    "    qz = inference_model(x_in, seq_mask, seq_len)\n",
    "    z = qz.mean()\n",
    "    reconstruction = greedy_decode(model, z, vocab)\n",
    "    reconstruction = batch_to_words(reconstruction, vocab)[0]\n",
    "    print(\"-- Original word: \\\"%s\\\"\" % random_word)\n",
    "    print(\"-- Model reconstruction: \\\"%s\\\"\" % reconstruction)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "h2GpYHvqScSK"
   },
   "source": [
    "# Let's plot the training and validation statistics:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "IYnp8E4MScSL"
   },
   "outputs": [],
   "source": [
    "steps, training_ELBO = list(zip(*train_ELBOs))\n",
    "_, training_KL = list(zip(*train_KLs))\n",
    "_, val_ELBO = list(zip(*val_ELBOs))\n",
    "_, val_KL = list(zip(*val_KLs))\n",
    "epochs, val_ppl = list(zip(*val_perplexities))\n",
    "_, val_NLL = list(zip(*val_NLLs))\n",
    "\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(18, 5))\n",
    "\n",
    "# Plot training ELBO and KL\n",
    "ax1.set_title(\"Training ELBO\")\n",
    "ax1.plot(steps, training_ELBO, \"-o\")\n",
    "ax2.set_title(\"Training KL\")\n",
    "ax2.plot(steps, training_KL, \"-o\")\n",
    "plt.show()\n",
    "\n",
    "# Plot validation ELBO and KL\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(18, 5))\n",
    "ax1.set_title(\"Validation ELBO\")\n",
    "ax1.plot(steps, val_ELBO, \"-o\", color=\"orange\")\n",
    "ax2.set_title(\"Validation KL\")\n",
    "ax2.plot(steps, val_KL, \"-o\",  color=\"orange\")\n",
    "plt.show()\n",
    "\n",
    "# Plot validation perplexities.\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(18, 5))\n",
    "ax1.set_title(\"Validation perplexity\")\n",
    "ax1.plot(epochs, val_ppl, \"-o\", color=\"orange\")\n",
    "ax2.set_title(\"Validation NLL\")\n",
    "ax2.plot(epochs, val_NLL, \"-o\",  color=\"orange\")\n",
    "plt.show()\n",
    "print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "AF4L6yyKScSP"
   },
   "source": [
    "Let's load the best model according to validation perplexity and compute its perplexity on the test data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "K25svlmSScSQ",
    "outputId": "4d414aa5-46d7-4116-cd2f-52bc314edd3d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test ELBO (KL) = -25.34 (5.46) -- test perplexity = 9.56 -- test NLL = 24.05\n"
     ]
    }
   ],
   "source": [
    "# Load the best model from disk.\n",
    "model = LatentFactorModel(vocab_size=vocab.size(), \n",
    "                 emb_size=emb_size, \n",
    "                 hidden_size=hidden_size, \n",
    "                 latent_size=latent_size, \n",
    "                 pad_idx=vocab[PAD_TOKEN],\n",
    "                 dropout=dropout)\n",
    "model.load_state_dict(torch.load(best_model))\n",
    "model = model.to(device)\n",
    "\n",
    "# Compute test perplexity and ELBO.\n",
    "test_perplexity, test_NLL = eval_perplexity(model, inference_model, test_dataset, vocab, \n",
    "                                            device, n_importance_samples)\n",
    "test_ELBO, test_KL = eval_elbo(model, inference_model, test_dataset, vocab, device)\n",
    "print(\"test ELBO (KL) = %.2f (%.2f) -- test perplexity = %.2f -- test NLL = %.2f\" % \n",
    "      (test_ELBO, test_KL, test_perplexity, test_NLL))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "DlxoPIa_ScSS"
   },
   "source": [
    "# Qualitative analysis\n",
    "\n",
    "Let's have a look at what how our trained model interacts with the learned latent space. First let's greedily decode some samples from the prior to assess the diversity of the model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "1x9siL9aScST"
   },
   "outputs": [],
   "source": [
    "# Generate 10 samples from the standard normal prior.\n",
    "num_prior_samples = 10\n",
    "pz = ProductOfBernoullis(torch.ones(num_prior_samples, latent_size) * 0.5)\n",
    "z = pz.sample()\n",
    "z = z.to(device)\n",
    "\n",
    "# Use the greedy decoding algorithm to generate words.\n",
    "predictions = greedy_decode(model, z, vocab)\n",
    "predictions = batch_to_words(predictions, vocab)\n",
    "for num, prediction in enumerate(predictions):\n",
    "    print(\"%d: %s\" % (num+1, prediction))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "O7T4hLydScSV"
   },
   "source": [
    "Let's now have a look how good the model is at reconstructing words from the test dataset using the approximate posterior mean and a couple of samples:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "6fi-g4ASScSW"
   },
   "outputs": [],
   "source": [
    "# Pick a random test word.\n",
    "test_word = test_dataset[np.random.choice(len(test_dataset))]\n",
    "\n",
    "# Infer q(z|x).\n",
    "x_in, _, seq_mask, seq_len = create_batch([test_word], vocab, device)\n",
    "qz = inference_model(x_in, seq_mask, seq_len)\n",
    "\n",
    "# Decode using the mean.\n",
    "z_mean = qz.mean()\n",
    "mean_reconstruction = greedy_decode(model, z_mean, vocab)\n",
    "mean_reconstruction = batch_to_words(mean_reconstruction, vocab)[0]\n",
    "\n",
    "print(\"Original: \\\"%s\\\"\" % test_word)\n",
    "print(\"Posterior mean reconstruction: \\\"%s\\\"\" % mean_reconstruction)\n",
    "\n",
    "# Decode a couple of samples from the approximate posterior.\n",
    "for s in range(3):\n",
    "    z = qz.sample()\n",
    "    sample_reconstruction = greedy_decode(model, z, vocab)\n",
    "    sample_reconstruction = batch_to_words(sample_reconstruction, vocab)[0]\n",
    "    print(\"Posterior sample reconstruction (%d): \\\"%s\\\"\" % (s+1, sample_reconstruction))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "fi0THhQ5ScSZ"
   },
   "source": [
    "We can also qualitatively assess the smoothness of the learned latent space by interpolating between two words in the test set:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "y8Ai6FxnScSZ"
   },
   "outputs": [],
   "source": [
    "# Pick a random test word.\n",
    "test_word_1 = test_dataset[np.random.choice(len(test_dataset))]\n",
    "\n",
    "# Infer q(z|x).\n",
    "x_in, _, seq_mask, seq_len = create_batch([test_word_1], vocab, device)\n",
    "qz = inference_model(x_in, seq_mask, seq_len)\n",
    "qz_1 = qz.mean()\n",
    "\n",
    "# Pick a random second test word.\n",
    "test_word_2 = test_dataset[np.random.choice(len(test_dataset))]\n",
    "\n",
    "# Infer q(z|x) again.\n",
    "x_in, _, seq_mask, seq_len = create_batch([test_word_2], vocab, device)\n",
    "qz = inference_model(x_in, seq_mask, seq_len)\n",
    "qz_2 = qz.mean()\n",
    "\n",
    "# Now interpolate between the two means and generate words between those.\n",
    "num_words = 5\n",
    "print(\"Word 1: \\\"%s\\\"\" % test_word_1)\n",
    "for alpha in np.linspace(start=0., stop=1., num=num_words):\n",
    "    z = (1-alpha) * qz_1 + alpha * qz_2\n",
    "    reconstruction = greedy_decode(model, z, vocab)\n",
    "    reconstruction = batch_to_words(reconstruction, vocab)[0]\n",
    "    print(\"(1-%.2f) * qz1.mean + %.2f qz2.mean: \\\"%s\\\"\" % (alpha, alpha, reconstruction))\n",
    "print(\"Word 2: \\\"%s\\\"\" % test_word_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "C_dLVVbiScSd"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "include_colab_link": true,
   "name": "LatentFactorModel-Solutions.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
