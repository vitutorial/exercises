{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "view-in-github"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/Roxot/vitutorial-exercises/blob/master/CharLM.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "AcYW7MDKScQ6"
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import os\n",
    "import re\n",
    "import urllib.request\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import matplotlib.pyplot as plt\n",
    "import itertools\n",
    "\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.nn.utils.rnn import pack_padded_sequence, pad_packed_sequence\n",
    "device = torch.device(\"cuda:0\") if torch.cuda.is_available() else torch.device(\"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "pMktCZC3Vwyg"
   },
   "source": [
    "In this notebook you will work with a generative language model that generates inflected wordforms one character at a time. We will use text data (we will work on the character level) in Spanish and pytorch. \n",
    "\n",
    "The first section concerns data manipulation and data loading classes necessary for our implementation. You do not need to modify anything in this part of the code."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ca_L1zfDScRC"
   },
   "source": [
    "Let's first download the SIGMORPHON dataset that we will be using for this notebook: these are inflected Spanish words together with some morphosyntactic descriptors. For this notebook we will ignore the morphosyntactic descriptors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 51
    },
    "colab_type": "code",
    "id": "pYl0oQJVScRE",
    "outputId": "3dd12bc6-9380-4cda-ecbd-dfc1eadda9ff"
   },
   "outputs": [],
   "source": [
    "url = \"https://raw.githubusercontent.com/ryancotterell/sigmorphon2016/master/data/\"\n",
    "train_file = \"spanish-task1-train\"\n",
    "val_file = \"spanish-task1-dev\"\n",
    "test_file = \"spanish-task1-test\"\n",
    "\n",
    "print(\"Downloading data files...\")\n",
    "if not os.path.isfile(train_file):\n",
    "    urllib.request.urlretrieve(url + train_file, filename=train_file)\n",
    "if not os.path.isfile(val_file):\n",
    "    urllib.request.urlretrieve(url + val_file, filename=val_file)\n",
    "if not os.path.isfile(test_file):\n",
    "    urllib.request.urlretrieve(url + test_file, filename=test_file)\n",
    "print(\"Download complete.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "oPlLBwayScRN"
   },
   "source": [
    "# Data\n",
    "\n",
    "In order to work with text data, we need to transform the text into something that our algorithms can work with. The first step of this process is converting words into word ids. We do this by constructing a vocabulary from the data, assigning a new word id to each new word it encounters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "S4wtwvwaScRP"
   },
   "outputs": [],
   "source": [
    "UNK_TOKEN = \"?\"\n",
    "PAD_TOKEN = \"_\"\n",
    "SOW_TOKEN = \">\"\n",
    "EOW_TOKEN = \".\"\n",
    "\n",
    "def extract_inflected_word(s):\n",
    "    \"\"\"\n",
    "    Extracts the inflected words in the SIGMORPHON dataset.\n",
    "    \"\"\"\n",
    "    return s.split()[-1]\n",
    "\n",
    "class Vocabulary:\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.idx_to_char = {0: UNK_TOKEN, 1: PAD_TOKEN, 2: SOW_TOKEN, 3: EOW_TOKEN}\n",
    "        self.char_to_idx = {UNK_TOKEN: 0, PAD_TOKEN: 1, SOW_TOKEN: 2, EOW_TOKEN: 3}\n",
    "        self.word_freqs = {}\n",
    "    \n",
    "    def __getitem__(self, key):\n",
    "        return self.char_to_idx[key] if key in self.char_to_idx else self.char_to_idx[UNK_TOKEN]\n",
    "    \n",
    "    def word(self, idx):\n",
    "        return self.idx_to_char[idx]\n",
    "    \n",
    "    def size(self):\n",
    "        return len(self.char_to_idx)\n",
    "    \n",
    "    @staticmethod\n",
    "    def from_data(filenames):\n",
    "        \"\"\"\n",
    "            Creates a vocabulary from a list of data files. It assumes that the data files have been\n",
    "            tokenized and pre-processed beforehand.\n",
    "        \"\"\"\n",
    "        vocab = Vocabulary()\n",
    "        for filename in filenames:\n",
    "            with open(filename) as f:\n",
    "                for line in f:\n",
    "                    \n",
    "                    # Strip whitespace and the newline symbol.\n",
    "                    word = extract_inflected_word(line.strip())\n",
    "                    \n",
    "                    # Split the words into characters and assign ids to each\n",
    "                    # new character it encounters.\n",
    "                    for char in list(word):\n",
    "                        if char not in vocab.char_to_idx:\n",
    "                            idx = len(vocab.char_to_idx)\n",
    "                            vocab.char_to_idx[char] = idx\n",
    "                            vocab.idx_to_char[idx] = char\n",
    "                            \n",
    "        return vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 51
    },
    "colab_type": "code",
    "id": "5yGmXEQ2TQSJ",
    "outputId": "6f33a3ea-6478-4dce-f70a-56f3730d6c6e"
   },
   "outputs": [],
   "source": [
    "# Construct a vocabulary from the training and validation data.\n",
    "print(\"Constructing vocabulary...\")\n",
    "vocab = Vocabulary.from_data([train_file, val_file])\n",
    "print(\"Constructed a vocabulary of %d types\" % vocab.size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 68
    },
    "colab_type": "code",
    "id": "jLP1Yy2_TtzR",
    "outputId": "3e990960-c9be-40a5-f7c8-8f0819956a9c"
   },
   "outputs": [],
   "source": [
    "# some examples\n",
    "print('e', vocab['e'])\n",
    "print('é', vocab['é'])\n",
    "print('ș', vocab['ș'])  # something UNKNOWN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "0Wwdq1DtScRX"
   },
   "source": [
    "We also need to load the data files into memory. We create a simple class `TextDataset` that stores the data as a list of words:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "JYC5fXfIScRa"
   },
   "outputs": [],
   "source": [
    "class TextDataset(Dataset):\n",
    "    \"\"\"\n",
    "        A simple class that loads a list of words into memory from a text file,\n",
    "        split by newlines. This does not do any memory optimisation, \n",
    "        so if your dataset is very large, you might want to use an alternative \n",
    "        class.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, text_file, max_len=30):\n",
    "        self.data = []\n",
    "        with open(text_file) as f:\n",
    "            for line in f:\n",
    "                word = extract_inflected_word(line.strip())\n",
    "                if len(list(word)) <= max_len:\n",
    "                    self.data.append(word)\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        return self.data[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 68
    },
    "colab_type": "code",
    "id": "SehavEDQTfIe",
    "outputId": "e6b2d588-cc74-47a9-a4c9-358cb1079bd6"
   },
   "outputs": [],
   "source": [
    "# Load the training, validation, and test datasets into memory.\n",
    "train_dataset = TextDataset(train_file)\n",
    "val_dataset = TextDataset(val_file)\n",
    "test_dataset = TextDataset(test_file)\n",
    "\n",
    "# Print some samples from the data:\n",
    "print(\"Sample from training data: \\\"%s\\\"\" % train_dataset[np.random.choice(len(train_dataset))])\n",
    "print(\"Sample from validation data: \\\"%s\\\"\" % val_dataset[np.random.choice(len(val_dataset))])\n",
    "print(\"Sample from test data: \\\"%s\\\"\" % test_dataset[np.random.choice(len(test_dataset))])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "PNQ1TG-3ScRh"
   },
   "source": [
    "Now it's time to write a function that converts a word into a list of character ids using the vocabulary we created before. This function is `create_batch` in the code cell below. This function creates a batch from a list of words, and makes sure that each word starts with a start-of-word symbol and ends with an end-of-word symbol. Because not all words are of equal length in a certain batch, words are padded with padding symbols so that they match the length of the largest word in the batch. The function returns an input batch, an output batch, a mask of 1s for words and 0s for padding symbols, and the sequence lengths of each word in the batch. The output batch is shifted by one character, to reflect the predictions that the model is expected to make. For example, for a word\n",
    "\\begin{align}\n",
    "    \\text{e s p e s e m o s}\n",
    "\\end{align}\n",
    "the input sequence is\n",
    "\\begin{align}\n",
    "    \\text{SOW e s p e s e m o s}\n",
    "\\end{align}\n",
    "and the output sequence is\n",
    "\\begin{align}\n",
    "    \\text{e s p e s e m o s EOW}\n",
    "\\end{align}\n",
    "\n",
    "You can see the output is shifted wrt the input, that's because we will be computing a distribution for the next character in context of its prefix, and that's why we need to shift the sequence this way.\n",
    "\n",
    "\n",
    "Lastly, we create an inverse function `batch_to_words` that recovers the list of words from a padded batch of character ids to use during test time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "GcYfsIaBScRj"
   },
   "outputs": [],
   "source": [
    "def create_batch(words, vocab, device, word_dropout=0.):\n",
    "    \"\"\"\n",
    "    Converts a list of words to a padded batch of word ids. Returns\n",
    "    an input batch, an output batch shifted by one, a sequence mask over\n",
    "    the input batch, and a tensor containing the sequence length of each\n",
    "    batch element.\n",
    "    :param words: a list of words, each a list of token ids\n",
    "    :param vocab: a Vocabulary object for this dataset\n",
    "    :param device: \n",
    "    :param word_dropout: rate at which we omit words from the context (input)\n",
    "    :returns: a batch of padded inputs, a batch of padded outputs, mask, lengths\n",
    "    \"\"\"\n",
    "    tok = np.array([[SOW_TOKEN] + list(w) + [EOW_TOKEN] for w in words])\n",
    "    seq_lengths = [len(w)-1 for w in tok]\n",
    "    max_len = max(seq_lengths)\n",
    "    pad_id = vocab[PAD_TOKEN]\n",
    "    pad_id_input = [\n",
    "        [vocab[w[t]] if t < seq_lengths[idx] else pad_id for t in range(max_len)]\n",
    "            for idx, w in enumerate(tok)]\n",
    "    \n",
    "    # Replace words of the input with <unk> with p = word_dropout.\n",
    "    if word_dropout > 0.:\n",
    "        unk_id = vocab[UNK_TOKEN]\n",
    "        word_drop =  [\n",
    "            [unk_id if (np.random.random() < word_dropout and t < seq_lengths[idx]) else word_ids[t] for t in range(max_len)] \n",
    "                for idx, word_ids in enumerate(pad_id_input)]\n",
    "    \n",
    "    # The output batch is shifted by 1.\n",
    "    pad_id_output = [\n",
    "        [vocab[w[t+1]] if t < seq_lengths[idx] else pad_id for t in range(max_len)]\n",
    "            for idx, w in enumerate(tok)]\n",
    "    \n",
    "    # Convert everything to PyTorch tensors.\n",
    "    batch_input = torch.tensor(pad_id_input)\n",
    "    batch_output = torch.tensor(pad_id_output)\n",
    "    seq_mask = (batch_input != vocab[PAD_TOKEN])\n",
    "    seq_length = torch.tensor(seq_lengths)\n",
    "    \n",
    "    # Move all tensors to the given device.\n",
    "    batch_input = batch_input.to(device)\n",
    "    batch_output = batch_output.to(device)\n",
    "    seq_mask = seq_mask.to(device)\n",
    "    seq_length = seq_length.to(device)\n",
    "    \n",
    "    return batch_input, batch_output, seq_mask, seq_length\n",
    "\n",
    "\n",
    "def batch_to_words(tensors, vocab: Vocabulary):\n",
    "    \"\"\"\n",
    "    Converts a batch of word ids back to words.\n",
    "    :param tensors: [B, T] word ids\n",
    "    :param vocab: a Vocabulary object for this dataset\n",
    "    :returns: an array of strings (each a word).\n",
    "    \"\"\"\n",
    "    words = []\n",
    "    batch_size = tensors.size(0)\n",
    "    for idx in range(batch_size):\n",
    "        word = [vocab.word(t.item()) for t in tensors[idx,:]]\n",
    "        \n",
    "        # Filter out the start-of-word and padding tokens.\n",
    "        word = list(filter(lambda t: t != PAD_TOKEN and t != SOW_TOKEN, word))\n",
    "        \n",
    "        # Remove the end-of-word token and all tokens following it.\n",
    "        if EOW_TOKEN in word:\n",
    "            word = word[:word.index(EOW_TOKEN)]\n",
    "            \n",
    "        words.append(\"\".join(word))\n",
    "    return np.array(words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "l7R5_4wwScRq"
   },
   "source": [
    "In PyTorch the RNN functions expect inputs to be sorted from long words to shorter ones. Therefore we create a simple wrapper class for the DataLoader class that sorts words from long to short:  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Xjtwes5iScRs"
   },
   "outputs": [],
   "source": [
    "class SortingTextDataLoader:\n",
    "    \"\"\"\n",
    "    A wrapper for the DataLoader class that sorts a list of words by their\n",
    "    lengths in descending order.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, dataloader):\n",
    "        self.dataloader = dataloader\n",
    "        self.it = iter(dataloader)\n",
    "    \n",
    "    def __iter__(self):\n",
    "        return self\n",
    "    \n",
    "    def __next__(self):\n",
    "        words = None\n",
    "        for s in self.it:\n",
    "            words = s\n",
    "            break\n",
    "\n",
    "        if words is None:\n",
    "            self.it = iter(self.dataloader)\n",
    "            raise StopIteration\n",
    "        \n",
    "        words = np.array(words)\n",
    "        sort_keys = sorted(range(len(words)), \n",
    "                           key=lambda idx: len(list(words[idx])), \n",
    "                           reverse=True)\n",
    "        sorted_words = words[sort_keys]\n",
    "        return sorted_words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "bkrOx_-NScRw"
   },
   "source": [
    "# Model\n",
    "\n",
    "## Deterministic language model\n",
    "\n",
    "In language modelling, we model a word $x = \\langle x_1, \\ldots, x_n \\rangle$  of length $n = |x|$ as a sequence of categorical draws:\n",
    "\n",
    "\\begin{align}\n",
    "X_i|x_{<i} & \\sim \\text{Cat}(f(x_{<i}; \\theta)) \n",
    "& i = 1, \\ldots, n \\\\\n",
    "\\end{align}\n",
    "\n",
    "where we use $x_{<i}$ to denote a (possibly empty) prefix string, and thus the model makes no Markov assumption. We map from the conditioning context, the prefix $x_{<i}$, to the categorical parameters (a $v$-dimensional probability vector, where $v$ denotes the size of the vocabulary, in this case, the size of the character set) using a fixed neural network architecture whose parameters we collectively denote by $\\theta$.\n",
    "\n",
    "This assigns the following likelihood to the word\n",
    "\\begin{align}\n",
    "    P(x|\\theta) &= \\prod_{i=1}^n P(x_i|x_{<i}, \\theta) \\\\\n",
    "    &= \\prod_{i=1}^n \\text{Cat}(x_i|f(x_{<i}; \\theta))  \n",
    "\\end{align}\n",
    "where the categorical pmf is $\\text{Cat}(k|\\pi) = \\prod_{j=1}^v \\pi_j^{[k=j]} = \\pi_k$. \n",
    "\n",
    "\n",
    "Suppose we have a dataset $\\mathcal D = \\{x^{(1)}, \\ldots, x^{(N)}\\}$ containing $N$ i.i.d. observations. Then we can use the log-likelihood function \n",
    "\\begin{align}\n",
    "\\mathcal L(\\theta|\\mathcal D) &= \\sum_{k=1}^{N} \\log P(x^{(k)}| \\theta) \\\\\n",
    "&= \\sum_{k=1}^{N} \\sum_{i=1}^{|x^{(k)}|} \\log \\text{Cat}(x^{(k)}_i|f(x^{(k)}_{<i}; \\theta))\n",
    "\\end{align}\n",
    " to estimate $\\theta$ by maximisation:\n",
    " \\begin{align}\n",
    " \\theta^\\star = \\arg\\max_{\\theta \\in \\Theta} \\mathcal L(\\theta|\\mathcal D) ~ .\n",
    " \\end{align}\n",
    " \n",
    "\n",
    "We can use stochastic gradient-ascent to find a local optimum of $\\mathcal L(\\theta|\\mathcal D)$, which only requires a gradient estimate:\n",
    "\n",
    "\\begin{align}\n",
    "\\nabla_\\theta \\mathcal L(\\theta|\\mathcal D) &= \\sum_{k=1}^{|\\mathcal D|} \\nabla_\\theta  \\log P(x^{(k)}|\\theta) \\\\ \n",
    "&= \\sum_{k=1}^{|\\mathcal D|} \\frac{1}{N} N \\nabla_\\theta  \\log P(x^{(k)}| \\theta)  \\\\\n",
    "&= \\mathbb E_{\\mathcal U(1/N)} \\left[ N \\nabla_\\theta  \\log P(x^{(K)}| \\theta) \\right]  \\\\\n",
    "&\\overset{\\text{MC}}{\\approx} \\frac{N}{M} \\sum_{m=1}^M \\nabla_\\theta  \\log P(x^{(k_m)}|\\theta) \\\\\n",
    "&\\text{where }K_m \\sim \\mathcal U(1/N)\n",
    "\\end{align}\n",
    "\n",
    "This is a Monte Carlo (MC) estimate of the gradient computed on $M$ data points selected uniformly at random from $\\mathcal D$.\n",
    "\n",
    "For as long as $f$ remains differentiable wrt to its inputs and parameters, we can rely on automatic differentiation to obtain gradient estimates.\n",
    "\n",
    "\n",
    "An example design for $f$ is:\n",
    "\\begin{align}\n",
    "\\mathbf x_i &= \\text{emb}(x_i; \\theta_{\\text{emb}}) \\\\\n",
    "\\mathbf h_0 &= \\mathbf 0 \\\\\n",
    "\\mathbf h_i &= \\text{rnn}(\\mathbf h_{i-1}, \\mathbf x_{i-1}; \\theta_{\\text{rnn}}) \\\\\n",
    "f(x_{<i}; \\theta) &= \\text{softmax}(\\text{dense}_v(\\mathbf h_{i};  \\theta_{\\text{out}}))\n",
    "\\end{align}\n",
    "where \n",
    "* $\\text{emb}$ is a fixed embedding layer with parameters $\\theta_{\\text{emb}}$;\n",
    "* $\\text{rnn}$ is a recurrent architecture with parameters $\\theta_{\\text{rnn}}$, e.g. an LSTM or GRU, and $\\mathbf h_0$ is part of the architecture's parameters;\n",
    "* $\\text{dense}_v$ is a dense layer with $v$ outputs (vocabulary size) and parameters $\\theta_{\\text{out}}$.\n",
    "\n",
    "\n",
    "\n",
    "In follow up labs this model will be a component in a *deep generative model*. You may skip this lab if you are comfortable with the overall notion of having neural networks parameterise statistical/probabilistic models."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "A6lhOGrkTNA3"
   },
   "source": [
    "\n",
    "## Implementation\n",
    "\n",
    "In our implementation we create a pytorch module with the NN blocks we need, namely, embedding layer, LSTM cell, and output layer. The forward method will use these layers to map inputs to categorical distributions. \n",
    "\n",
    "You may want to check the documentation for\n",
    "\n",
    "* `torch.nn.Embedding`\n",
    "* `torch.nn.Linear`\n",
    "* `torch.nn.LSTM`\n",
    "* `torch.distributions.categorical.Categorical`\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "LXXS-JDbpRND"
   },
   "outputs": [],
   "source": [
    "from torch.distributions import Categorical\n",
    "\n",
    "class CharLM(nn.Module):\n",
    "    \n",
    "    def __init__(self, vocab_size, emb_size, hidden_size,\n",
    "                 pad_idx, dropout=0.):\n",
    "        \"\"\"\n",
    "        :param vocab_size: size of the vocabulary of the language\n",
    "        :param emb_size: dimensionality of embeddings\n",
    "        :param hidden_size: dimensionality of recurrent cell        \n",
    "        :param pad_idx: the id reserved to the -PAD- token\n",
    "        :param dropout: a dropout rate (you can ignore this for now)\n",
    "        \"\"\"\n",
    "        super().__init__()  # necessary for pytorch\n",
    "        # Construct your NN architecture here\n",
    "        # - in pytorch your NN blocks must be attributes of self, \n",
    "        #   e.g. self.linear_layer = nn.Linear(...)\n",
    "        pass\n",
    "\n",
    "    def step(self, prev_x, hidden):\n",
    "        \"\"\"\n",
    "        Performs a single LSTM step for a given previous word and hidden state.\n",
    "        Returns the unnormalized log probabilities (logits) over the vocabulary for this time step\n",
    "        and the updated hidden state.\n",
    "\n",
    "        :param prev_x: [B, 1] id of the previous token        \n",
    "        :param hidden:  hidden ([num_layers, B, H] state, [num_layers, B, H] cell)\n",
    "\n",
    "        :returns: [B, 1, V], ([num_layers, B, H], [num_layers, B, H])\n",
    "        \"\"\"\n",
    "        # output: [B, H]\n",
    "        return None, None\n",
    "\n",
    "    def forward(self, x) -> Categorical:\n",
    "        \"\"\"\n",
    "        Performs an entire forward pass given a sequence of words x.\n",
    "        Returns a collection of Categorical distributions over the vocabulary, \n",
    "        one distribution per time step per sequence. \n",
    "\n",
    "        :param x: [B, T] token ids \n",
    "\n",
    "        Returns a [B,T] collection of Categorical distributions, each over V outcomes.\n",
    "        \"\"\"        \n",
    "        # Setting hidden=None implies a fixed initial hidden/cell state, namely, a vector of zeros.\n",
    "        hidden = None\n",
    "        outputs = []\n",
    "        for t in range(x.size(1)):\n",
    "            # Here we slice the t-th token\n",
    "            # [B, 1]\n",
    "            prev_x = x[:, t].unsqueeze(-1)\n",
    "            # Here we update our hidden state for prediction of \n",
    "            #  a categorical distribution for outcome at position t+1\n",
    "            # logits: [B, 1, V]\n",
    "            logits, hidden = self.step(prev_x, hidden)\n",
    "            outputs.append(logits)\n",
    "        # [B, T, V]\n",
    "        outputs = torch.cat(outputs, dim=1)\n",
    "        return Categorical(logits=outputs)\n",
    "        \n",
    "    def loss(self, output_distributions: Categorical, observations):\n",
    "        \"\"\"\n",
    "        Computes the terms in the loss (negative log-likelihood) given a \n",
    "        collection of Categorical output distributions and a collection of\n",
    "        observations in context (i.e. x_i|x_{<i}).\n",
    "        \n",
    "        :params output_distributions: a [B, T] collection of V-dimensional\n",
    "            categorical distributions.\n",
    "        :params observations: [B, T] token ids (next tokens, with respect to \n",
    "            the inputs that were used to parameterise the output distributions)\n",
    "        :returns: \n",
    "            negative log likelihood (scalar node), dict reserved for future use\n",
    "        \"\"\"        \n",
    "        return None, dict()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "53Aztjb5ScR2"
   },
   "source": [
    "The code below is used to assess the model and also investigate what it learned. We implemented it for you. It's useful however to learn from this example: we do interesting things like computing perplexity and sampling novel words!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "PLfa31jEScR7"
   },
   "source": [
    "# Evaluation metrics\n",
    "\n",
    "During training we'd like to keep track of some evaluation metrics on the validation data in order to keep track of how our model is doing and to perform early stopping. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "tGO7RBYgScR_"
   },
   "source": [
    "\n",
    "A common metric to evaluate language models is the perplexity per word. The perplexity per word for a dataset is defined as:\n",
    "\n",
    "\\begin{align}\n",
    "    \\text{ppl}(\\mathcal{D}|\\theta) = \\exp\\left(-\\frac{1}{\\sum_{k=1}^{|\\mathcal D|} n^{(k)}} \\sum_{k=1}^{|\\mathcal{D}|} \\log P(x^{(k)}|\\theta)\\right) \n",
    "\\end{align}\n",
    "\n",
    "where $n^{(k)} = |x^{(k)}|$ is the number of tokens in a word and $P(x^{(k)})$ is the probability that our model assigns to the datapoint $x^{(k)}$. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "VnaIidkwScSA"
   },
   "outputs": [],
   "source": [
    "def eval_perplexity(model, dataset, vocab, device, batch_size=128):\n",
    "    \"\"\"\n",
    "    Return ppl per word as well as average negative log-likelihood\n",
    "        given a model and a dataset.\n",
    "    \"\"\"    \n",
    "    dl = DataLoader(dataset, batch_size=batch_size)\n",
    "    sorted_dl = SortingTextDataLoader(dl)\n",
    "    \n",
    "    # Make sure the model is in evaluation mode (i.e. disable dropout).\n",
    "    model.eval()\n",
    "    \n",
    "    log_px = 0.\n",
    "    num_predictions = 0\n",
    "    num_words = 0\n",
    "     \n",
    "    # We don't need to compute gradients for this.\n",
    "    with torch.no_grad():\n",
    "        for words in sorted_dl:\n",
    "            # [B, T], [B, T], [B, T], [B]\n",
    "            x_in, x_out, seq_mask, seq_len = create_batch(words, vocab, device)\n",
    "            \n",
    "            # Compute output categoricals, i.e.\n",
    "            #  X_i|theta, x_{<i} ~ Cat(f(x_{<i}; theta))\n",
    "            p_x = model(x_in) \n",
    "\n",
    "            # [B, T]\n",
    "            log_prob = p_x.log_prob(x_out)\n",
    "            # Let's mask positions that should not contribute to the likelihood\n",
    "            log_prob = torch.where(seq_mask, log_prob, torch.zeros_like(log_prob))\n",
    "            # Let's aggregate the log-probability of all observations\n",
    "            # []\n",
    "            log_prob = log_prob.sum()\n",
    "            # And accumulate it over batches\n",
    "            log_px += log_prob\n",
    "            num_predictions += seq_len.sum()\n",
    "            num_words += seq_len.size(0)\n",
    "\n",
    "    # Compute and return the perplexity per word.\n",
    "    perplexity = torch.exp(-log_px / num_predictions)\n",
    "    NLL = -log_px / num_words\n",
    "    return perplexity, NLL"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "IvoVmedYScSC"
   },
   "source": [
    "Lastly, we want to occasionally qualitatively see the performance of the model during training, by letting it generate word forms from the trained model (a distribution over word forms). For this we use ancestral sampling, that's is we compute a categorical distribution for time step t, and draw a random outcome for that position, then include the random outcome in the available context for the next time step and repeat the process till we sample end of sequence or reach a pre-determined maximum length.\n",
    "\n",
    "If the model works well, this should mostly generate valid word forms, which means our model managed to concentrate probability mass on the space of valid Spanish words (rather than the full space of strings made of characters in the Spanish alphabet, which is a large set full of nonsense)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "LDD5XF1GScSC"
   },
   "outputs": [],
   "source": [
    "def ancestral_sampling(model, vocab, batch_size=None, max_len=50, device=None):\n",
    "    \"\"\"\n",
    "    Greedily decodes a word by picking the character with\n",
    "    maximum probability at each time step.\n",
    "    :params model: CharLM\n",
    "    :param vocab: a Vocabulary\n",
    "    :param max_len: cap on length for performance (GPU memory is not unbounded)\n",
    "\n",
    "    :return a batch of samples\n",
    "    \"\"\"\n",
    "    \n",
    "    # Disable dropout.\n",
    "    model.eval()\n",
    "\n",
    "    batch_size = 1 if batch_size is None else batch_size\n",
    "    \n",
    "    # Don't compute gradients.\n",
    "    with torch.no_grad():    \n",
    "        \n",
    "        # We feed the model the start-of-word symbol at the first time step.\n",
    "        prev_x = torch.ones(batch_size, 1, dtype=torch.long).fill_(vocab[SOW_TOKEN]).to(device)\n",
    "        \n",
    "        # Initialize the hidden state from z.\n",
    "        hidden = None\n",
    "\n",
    "        samples = []    \n",
    "        for t in range(max_len):\n",
    "            # logits: [B, V]\n",
    "            logits, hidden = model.step(prev_x, hidden=hidden)\n",
    "            # Cat(f(x_{<i}; theta))\n",
    "            p_x = Categorical(logits=logits)\n",
    "            # Random trial for X_i\n",
    "            # [B]\n",
    "            sample = p_x.sample()            \n",
    "            samples.append(sample)\n",
    "            # [B, 1]\n",
    "            prev_x = sample.view(batch_size, 1)\n",
    "        # [B, T]    \n",
    "        return torch.cat(samples, dim=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "GERuGgChScSE"
   },
   "source": [
    "# Training\n",
    "\n",
    "Now it's time to train the model. We use early stopping on the validation perplexity for model selection."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "XH6ocHxaScSF",
    "outputId": "7e2f2661-e743-46bb-e18d-cce5bc29a408"
   },
   "outputs": [],
   "source": [
    "# Define the model hyperparameters.\n",
    "emb_size = 256\n",
    "hidden_size = 256 \n",
    "dropout = 0.6\n",
    "batch_size = 64\n",
    "learning_rate = 0.001\n",
    "num_epochs = 20\n",
    "########\n",
    "\n",
    "# Create the training data loader.\n",
    "dl = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "sorted_dl = SortingTextDataLoader(dl)\n",
    "\n",
    "# Create the generative model.\n",
    "model = CharLM(\n",
    "    vocab_size=vocab.size(), \n",
    "    emb_size=emb_size, \n",
    "    hidden_size=hidden_size, \n",
    "    pad_idx=vocab[PAD_TOKEN],\n",
    "    dropout=dropout\n",
    ")\n",
    "model = model.to(device)\n",
    "\n",
    "# Create the optimizer.\n",
    "optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "# Save the best model (early stopping).\n",
    "best_model = \"./best_model.pt\"\n",
    "best_val_ppl = float(\"inf\")\n",
    "best_epoch = 0\n",
    "\n",
    "# Keep track of some statistics to plot later.\n",
    "train_NLLs = []\n",
    "val_perplexities = []\n",
    "val_NLLs = []\n",
    "\n",
    "step = 0\n",
    "training_loss = 0.\n",
    "num_batches = 0\n",
    "for epoch_num in range(1, num_epochs+1):    \n",
    "    for words in sorted_dl:\n",
    "\n",
    "        # Make sure the model is in training mode (for dropout).\n",
    "        model.train()\n",
    "\n",
    "        # Transform the words to input, output, seq_len, seq_mask batches.\n",
    "        x_in, x_out, seq_mask, seq_len = create_batch(words, vocab, device)\n",
    "\n",
    "        # Output distributions X_i|x_{<i}\n",
    "        p_x = model(x_in)\n",
    "        loss, _ = model.loss(p_x, x_out)\n",
    "\n",
    "        # Backpropagate and update the model weights.\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # Update some statistics to track for the training loss.\n",
    "        training_loss += loss        \n",
    "        num_batches += 1\n",
    "        \n",
    "        # Every 100 steps we evaluate the model and report progress.\n",
    "        if step % 100 == 0:\n",
    "            val_ppl, val_NLL = eval_perplexity(model, val_dataset, vocab, device)\n",
    "            print(\"(%d) step %d: training loss = %.2f --\"\n",
    "                  \" validation PPL = %.2f \" \n",
    "                  \" validation NLL = %.2f \" % \n",
    "                  (epoch_num, step, training_loss, \n",
    "                   val_ppl, val_NLL))\n",
    "            \n",
    "            # Update some statistics for plotting later.\n",
    "            train_NLLs.append((step, (training_loss/num_batches).item()))\n",
    "            val_perplexities.append((step, val_ppl.item()))\n",
    "            val_NLLs.append((step, val_NLL.item()))\n",
    "            \n",
    "            # Reset the training statistics.\n",
    "            training_loss = 0.            \n",
    "            num_batches = 0\n",
    "            \n",
    "        step += 1\n",
    "\n",
    "    # After an epoch we'll compute validation perplexity and save the model\n",
    "    # for early stopping if it's better than previous models.\n",
    "    print(\"Finished epoch %d\" % (epoch_num))\n",
    "    val_perplexity, val_NLL = eval_perplexity(model, val_dataset, vocab, device)    \n",
    "        \n",
    "    # If validation perplexity is better, store this model for early stopping.\n",
    "    if val_perplexity < best_val_ppl:\n",
    "        best_val_ppl = val_perplexity\n",
    "        best_epoch = epoch_num\n",
    "        torch.save(model.state_dict(), best_model)\n",
    "        \n",
    "    # Print epoch statistics.\n",
    "    print(\"Evaluation epoch %d:\\n\"\n",
    "          \" - validation perplexity: %.2f\\n\"\n",
    "          \" - validation NLL: %.2f\"\n",
    "          % (epoch_num, val_perplexity, val_NLL))\n",
    "\n",
    "    # Also show some qualitative results by sampling wordforms from the model.\n",
    "    samples = ancestral_sampling(model, vocab, batch_size=5, device=device)\n",
    "    for sample in batch_to_words(samples, vocab):        \n",
    "        print(\"-- Model generation: \\\"%s\\\"\" % sample)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "h2GpYHvqScSK"
   },
   "source": [
    "# Let's plot the training and validation statistics:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 672
    },
    "colab_type": "code",
    "id": "IYnp8E4MScSL",
    "outputId": "3a0ab663-160e-4f0a-d114-079b8d52b235"
   },
   "outputs": [],
   "source": [
    "steps, training_NLL = list(zip(*train_NLLs))\n",
    "_, val_ppl = list(zip(*val_perplexities))\n",
    "_, val_NLL = list(zip(*val_NLLs))\n",
    "\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(18, 5))\n",
    "\n",
    "# Plot training loss\n",
    "ax1.set_title(\"Training loss\")\n",
    "ax1.plot(steps, training_NLL, \"-o\")\n",
    "plt.show()\n",
    "\n",
    "# Plot validation perplexities.\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(18, 5))\n",
    "ax1.set_title(\"Validation perplexity\")\n",
    "ax1.plot(steps, val_ppl, \"-o\", color=\"orange\")\n",
    "ax2.set_title(\"Validation NLL\")\n",
    "ax2.plot(steps, val_NLL, \"-o\",  color=\"orange\")\n",
    "plt.show()\n",
    "print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "AF4L6yyKScSP"
   },
   "source": [
    "Let's load the best model according to validation perplexity and compute its perplexity on the test data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "K25svlmSScSQ",
    "outputId": "4c127176-9bda-4938-8e5b-f1002d0709c9"
   },
   "outputs": [],
   "source": [
    "# Load the best model from disk.\n",
    "model = CharLM(\n",
    "    vocab_size=vocab.size(), \n",
    "    emb_size=emb_size, \n",
    "    hidden_size=hidden_size,     \n",
    "    pad_idx=vocab[PAD_TOKEN],\n",
    "    dropout=dropout)\n",
    "model.load_state_dict(torch.load(best_model))\n",
    "model = model.to(device)\n",
    "\n",
    "# Compute test perplexity and ELBO.\n",
    "test_perplexity, test_NLL = eval_perplexity(model, test_dataset, vocab, device)\n",
    "print(\"test perplexity = %.2f -- test NLL = %.2f\" % \n",
    "      (test_perplexity, test_NLL))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "DlxoPIa_ScSS"
   },
   "source": [
    "# Qualitative analysis\n",
    "\n",
    "Let's generate some wordforms from our model and inspect what our model assigns non-negligible probability mass to."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 187
    },
    "colab_type": "code",
    "id": "1x9siL9aScST",
    "outputId": "9529f299-c5d6-44b4-fa77-d824be31bd37"
   },
   "outputs": [],
   "source": [
    "# Generate 10 samples from the standard normal prior.\n",
    "num_samples = 10\n",
    "\n",
    "samples = ancestral_sampling(model, vocab, batch_size=num_samples, device=device)\n",
    "for sample in batch_to_words(samples, vocab):        \n",
    "    print(\"-- Model generation: \\\"%s\\\"\" % sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "C_dLVVbiScSd"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "include_colab_link": true,
   "name": "CharLM.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
